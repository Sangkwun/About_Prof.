{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/seungheondoh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Author Keywords</th>\n",
       "      <th>Name</th>\n",
       "      <th>ResearchInterest</th>\n",
       "      <th>Title</th>\n",
       "      <th>_id</th>\n",
       "      <th>department</th>\n",
       "      <th>docs</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Entrepreneurs' decisions to exploit opportuni...</td>\n",
       "      <td>[nan, Behavioral decision-making; Business gro...</td>\n",
       "      <td>Choi, Young Rok(최영록)</td>\n",
       "      <td>Entrepreneurship, Innovation Strategy, Family ...</td>\n",
       "      <td>[Entrepreneurs' decisions to exploit opportuni...</td>\n",
       "      <td>5bf7bdc420c66c03397b5abd</td>\n",
       "      <td>Graduate School of Technology and Innovation M...</td>\n",
       "      <td>[[Entrepreneurs' decisions to exploit opportun...</td>\n",
       "      <td>Entrepreneurs' decisions to exploit opportuni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Organizations use project teams to lower sear...</td>\n",
       "      <td>[expertise awareness; expertise use; knowledge...</td>\n",
       "      <td>Hong, Woonki(홍운기)</td>\n",
       "      <td>Expertise utilization, Transactive menory syst...</td>\n",
       "      <td>[Explaining dyadic expertise use in knowledge ...</td>\n",
       "      <td>5bf7bdc520c66c03397b5abe</td>\n",
       "      <td>School of Business Administration(경영학부)</td>\n",
       "      <td>[[Explaining dyadic expertise use in knowledge...</td>\n",
       "      <td>Explaining dyadic expertise use in knowledge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[This paper investigates information leakage f...</td>\n",
       "      <td>[nan, nan, Business group; Chaebol; Control-ca...</td>\n",
       "      <td>Jung, Kooyul(정구열)</td>\n",
       "      <td>Accounting Information and capital market, Ana...</td>\n",
       "      <td>[Trading behavior prior to public release of a...</td>\n",
       "      <td>5bf7bdc520c66c03397b5abf</td>\n",
       "      <td>Graduate School of Technology and Innovation M...</td>\n",
       "      <td>[[Trading behavior prior to public release of ...</td>\n",
       "      <td>Trading behavior prior to public release of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Studies on the effects of corporate social re...</td>\n",
       "      <td>[annual reports; communication; corporate soci...</td>\n",
       "      <td>Kim, MinChung(김민중)</td>\n",
       "      <td>Marketing strategies and their impact on finan...</td>\n",
       "      <td>[CSR and Shareholder Value in the Restaurant I...</td>\n",
       "      <td>5bf7bdc520c66c03397b5ac0</td>\n",
       "      <td>School of Business Administration(경영학부)</td>\n",
       "      <td>[[CSR and Shareholder Value in the Restaurant ...</td>\n",
       "      <td>CSR and Shareholder Value in the Restaurant I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Brand communities and corporate social respon...</td>\n",
       "      <td>[Brand community; Consumer communities; Corpor...</td>\n",
       "      <td>Kim, Molan(김모란)</td>\n",
       "      <td>Online Database Marketing, Social Media Market...</td>\n",
       "      <td>[Consumer Communities Do Well, But Will They D...</td>\n",
       "      <td>5bf7bdc520c66c03397b5ac1</td>\n",
       "      <td>School of Business Administration(경영학부)</td>\n",
       "      <td>[[Consumer Communities Do Well, But Will They ...</td>\n",
       "      <td>Consumer Communities Do Well, But Will They D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  \\\n",
       "0  [Entrepreneurs' decisions to exploit opportuni...   \n",
       "1  [Organizations use project teams to lower sear...   \n",
       "2  [This paper investigates information leakage f...   \n",
       "3  [Studies on the effects of corporate social re...   \n",
       "4  [Brand communities and corporate social respon...   \n",
       "\n",
       "                                     Author Keywords                  Name  \\\n",
       "0  [nan, Behavioral decision-making; Business gro...  Choi, Young Rok(최영록)   \n",
       "1  [expertise awareness; expertise use; knowledge...     Hong, Woonki(홍운기)   \n",
       "2  [nan, nan, Business group; Chaebol; Control-ca...     Jung, Kooyul(정구열)   \n",
       "3  [annual reports; communication; corporate soci...    Kim, MinChung(김민중)   \n",
       "4  [Brand community; Consumer communities; Corpor...       Kim, Molan(김모란)   \n",
       "\n",
       "                                    ResearchInterest  \\\n",
       "0  Entrepreneurship, Innovation Strategy, Family ...   \n",
       "1  Expertise utilization, Transactive menory syst...   \n",
       "2  Accounting Information and capital market, Ana...   \n",
       "3  Marketing strategies and their impact on finan...   \n",
       "4  Online Database Marketing, Social Media Market...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  [Entrepreneurs' decisions to exploit opportuni...   \n",
       "1  [Explaining dyadic expertise use in knowledge ...   \n",
       "2  [Trading behavior prior to public release of a...   \n",
       "3  [CSR and Shareholder Value in the Restaurant I...   \n",
       "4  [Consumer Communities Do Well, But Will They D...   \n",
       "\n",
       "                        _id  \\\n",
       "0  5bf7bdc420c66c03397b5abd   \n",
       "1  5bf7bdc520c66c03397b5abe   \n",
       "2  5bf7bdc520c66c03397b5abf   \n",
       "3  5bf7bdc520c66c03397b5ac0   \n",
       "4  5bf7bdc520c66c03397b5ac1   \n",
       "\n",
       "                                          department  \\\n",
       "0  Graduate School of Technology and Innovation M...   \n",
       "1            School of Business Administration(경영학부)   \n",
       "2  Graduate School of Technology and Innovation M...   \n",
       "3            School of Business Administration(경영학부)   \n",
       "4            School of Business Administration(경영학부)   \n",
       "\n",
       "                                                docs  \\\n",
       "0  [[Entrepreneurs' decisions to exploit opportun...   \n",
       "1  [[Explaining dyadic expertise use in knowledge...   \n",
       "2  [[Trading behavior prior to public release of ...   \n",
       "3  [[CSR and Shareholder Value in the Restaurant ...   \n",
       "4  [[Consumer Communities Do Well, But Will They ...   \n",
       "\n",
       "                                                text  \n",
       "0   Entrepreneurs' decisions to exploit opportuni...  \n",
       "1   Explaining dyadic expertise use in knowledge ...  \n",
       "2   Trading behavior prior to public release of a...  \n",
       "3   CSR and Shareholder Value in the Restaurant I...  \n",
       "4   Consumer Communities Do Well, But Will They D...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "db = client.aboutdepart\n",
    "database = list(db.Docdb.find())\n",
    "testdf = pd.DataFrame(database)\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.to_csv('prof2vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "\n",
    "    docs = docs.lower()  # Convert to lowercase.\n",
    "    docs = tokenizer.tokenize(docs)  # Split into words.\n",
    "    docs = [w for w in docs if not w in stopwords.words('english')]\n",
    "    \n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [token for token in docs if not token.isdigit()]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [token for token in docs if len(token) > 3]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [lemmatizer.lemmatize(token) for token in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(testdf['text']):\n",
    "    add = docs_preprocessor(i)\n",
    "    testdf.iloc[idx]['text'] = add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(testdf['Name']):\n",
    "    testdf.iloc[idx]['Name'] = i.split(\"(\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata = zip(testdf['Name'],testdf['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in zipdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['shareholder', 'value', 'restaurant', 'industry', 'role', 'communication', 'annual', 'report', 'study', 'effect', 'corporate', 'social', 'responsibility', 'shareholder', 'value', 'identified', 'communication', 'marketing', 'channel', 'mechanism', 'necessary', 'translate', 'shareholder', 'value', 'study', 'annual', 'report', 'considered', 'primary', 'information', 'source', 'used', 'firm', 'financial', 'stakeholder', 'investor', 'analyst', 'valuation', 'firm', 'specifically', 'study', 'examined', 'whether', 'extent', 'restaurant', 'firm', 'communicate', 'annual', 'report', 'influence', 'effect', 'shareholder', 'value', 'return', 'tobin', 'risk', 'equity', 'holder', 'risk', 'disaggregated', 'distinct', 'type', 'primary', 'nonprimary', 'stakeholder', 'result', 'corroborate', 'communicating', 'nonprimary', 'stakeholder', 'annual', 'report', 'help', 'nonprimary', 'stakeholder', 'increase', 'shareholder', 'value', 'reducing', 'equity', 'holder', 'risk', 'whereas', 'communicating', 'primary', 'stakeholder', 'affect', 'value', 'relevance', 'primary', 'stakeholder', 'change', 'either', 'tobin', 'equity', 'holder', 'risk', 'author', 'annual', 'report', 'communication', 'corporate', 'social', 'responsibility', 'equity', 'holder', 'risk', 'tobin', 'effect', 'corporate', 'social', 'responsibility', 'corporate', 'financial', 'performance', 'competitive', 'action', 'perspective', 'attempt', 'provide', 'nuanced', 'view', 'relationship', 'corporate', 'social', 'responsibility', 'firm', 'financial', 'performance', 'using', 'competitive', 'action', 'perspective', 'argue', 'competitive', 'action', 'considered', 'important', 'contingency', 'determines', 'effect', 'activity', 'firm', 'financial', 'performance', 'using', 'data', 'publicly', 'listed', 'firm', 'software', 'industry', 'found', 'socially', 'responsible', 'activity', 'positive', 'enhance', 'firm', 'financial', 'performance', 'firm', 'competitive', 'action', 'level', 'high', 'whereas', 'socially', 'irresponsible', 'activity', 'negative', 'actually', 'improve', 'firm', 'financial', 'performance', 'competitive', 'action', 'level', 'introducing', 'competitive', 'action', 'important', 'contingency', 'study', 'contributes', 'literature', 'strategic', 'management', 'author', 'competitive', 'action', 'financial', 'performance', 'negative', 'positive', 'pride', 'lead', 'corporate', 'managerial', 'hubris', 'strategic', 'emphasis', 'firm', 'strategic', 'emphasis', 'value', 'creation', 'versus', 'appropriation', 'typically', 'reflected', 'resource', 'allocation', 'advertising', 'central', 'corporate', 'decision', 'significantly', 'influence', 'financial', 'performance', 'however', 'driver', 'decision', 'remain', 'underexplored', 'study', 'identifies', 'significant', 'predictor', 'strategic', 'emphasis', 'namely', 'corporate', 'managerial', 'hubris', 'reveals', 'boundary', 'condition', 'leveraging', 'unique', 'dataset', 'based', 'text', 'mining', 'press', 'release', 'issued', 'firm', 'across', 'year', 'author', 'demonstrate', 'high', 'corporate', 'managerial', 'hubris', 'predicts', 'strategic', 'emphasis', 'advertising', 'relative', 'however', 'effect', 'mitigated', 'significantly', 'firm', 'maturity', 'corporate', 'governance', 'industry', 'level', 'strategic', 'emphasis', 'result', 'provide', 'novel', 'insight', 'effect', 'hubris', 'firm', 'spending', 'situation', 'wherein', 'marketing', 'decision', 'tend', 'subject', 'manager', 'psychological', 'bias', 'mean', 'preventing', 'investment', 'marketing', 'strategy', 'recruitment', 'training', 'manager', 'academy', 'marketing', 'science', 'advertising', 'corporate', 'managerial', 'hubris', 'strategic', 'emphasis', 'corporate', 'social', 'responsibility', 'equity', 'holder', 'risk', 'hospitality', 'industry', 'study', 'examined', 'whether', 'corporate', 'social', 'responsibility', 'enhances', 'firm', 'value', 'shareholder', 'ultimately', 'fund', 'firm', 'initiative', 'specifically', 'investigated', 'relationship', 'activity', 'hospitality', 'firm', 'risk', 'associated', 'equity', 'holding', 'firm', 'using', 'msci', 'environmental', 'social', 'governance', 'rating', 'measured', 'extent', 'effort', 'firm', 'tested', 'effect', 'different', 'type', 'equity', 'holder', 'risk', 'systematic', 'unsystematic', 'risk', 'across', 'four', 'segment', 'hospitality', 'industry', 'airline', 'hotel', 'casino', 'restaurant', 'found', 'reduce', 'systematic', 'risk', 'restaurant', 'casino', 'firm', 'significantly', 'whereas', 'significant', 'influence', 'unsystematic', 'risk', 'segment', 'result', 'study', 'important', 'theoretical', 'practical', 'implication', 'academia', 'hospitality', 'industry', 'author', 'airline', 'casino', 'corporate', 'social', 'responsibility', 'equity', 'risk', 'hotel', 'restaurant', 'tourism', 'equity', 'incentive', 'shareholder', 'value', 'moderating', 'role', 'managerial', 'discretion', 'chief', 'marketing', 'officer', 'cmos', 'marketing', 'leader', 'respective', 'firm', 'responsible', 'creating', 'shareholder', 'value', 'incentivize', 'cmos', 'focus', 'shareholder', 'interest', 'increasing', 'number', 'firm', 'compensating', 'cmos', 'equity', 'study', 'examines', 'impact', 'equity', 'incentive', 'shareholder', 'value', 'introduces', 'three', 'different', 'form', 'managerial', 'discretion', 'given', 'cmos', 'contingency', 'determining', 'impact', 'equity', 'incentive', 'le', 'result', 'based', 'compensation', 'data', 'confirm', 'value', 'relevance', 'equity', 'incentive', 'beyond', 'equity', 'incentive', 'allocated', 'member', 'furthermore', 'result', 'reveal', 'support', 'moderating', 'effect', 'financial', 'strategic', 'operational', 'form', 'managerial', 'discretion', 'theoretically', 'finding', 'identify', 'hitherto', 'unrecognized', 'boundary', 'condition', 'determining', 'impact', 'equity', 'shareholder', 'value', 'higher', 'lower', 'address', 'several', 'limitation', 'characterizing', 'past', 'research', 'topic', 'managerially', 'result', 'guide', 'firm', 'structuring', 'compensation', 'discretion', 'way', 'optimize', 'effect', 'cmos', 'shareholder', 'value', 'elsevier', 'agency', 'problem', 'chief', 'marketing', 'officer', 'equity', 'incentive', 'equity', 'based', 'compensation', 'firm', 'value', 'managerial', 'discretion', 'corporate', 'social', 'responsibility', 'shareholder', 'value', 'restaurant', 'firm', 'response', 'recent', 'call', 'understanding', 'corporate', 'social', 'responsibility', 'influence', 'shareholder', 'value', 'study', 'examined', 'effect', 'strengthening', 'strengthening', 'firm', 'reputation', 'concerning', 'potentially', 'diminishing', 'reputation', 'activity', 'publicly', 'listed', 'restaurant', 'firm', 'shareholder', 'value', 'unlike', 'previous', 'study', 'hospitality', 'literature', 'focused', 'level', 'stock', 'return', 'study', 'considered', 'systematic', 'risk', 'another', 'important', 'determinant', 'shareholder', 'value', 'study', 'tested', 'whether', 'strengthening', 'concerning', 'action', 'restaurant', 'firm', 'associated', 'shareholder', 'value', 'based', 'systematic', 'risk', 'tobin', 'strengthening', 'action', 'found', 'enhance', 'shareholder', 'value', 'increasing', 'tobin', 'whereas', 'weakening', 'action', 'reduce', 'shareholder', 'value', 'increasing', 'systematic', 'risk', 'firm', 'analyzing', 'different', 'effect', 'strengthening', 'concerning', 'action', 'study', 'provides', 'interesting', 'important', 'implication', 'hospitality', 'literature', 'practitioner', 'restaurant', 'firm', 'elsevier', 'corporate', 'social', 'responsibility', 'hospitality', 'restaurant', 'shareholder', 'value', 'stock', 'return', 'systematic', 'risk', 'advertising', 'firm', 'risk', 'study', 'restaurant', 'industry', 'incorporating', 'recent', 'call', 'understanding', 'firm', 'equity', 'risk', 'relation', 'firm', 'marketing', 'effort', 'study', 'examined', 'impact', 'firm', 'level', 'advertising', 'spending', 'firm', 'equity', 'risk', 'publicly', 'listed', 'firm', 'restaurant', 'industry', 'hospitality', 'industry', 'study', 'hypothesized', 'tested', 'effect', 'firm', 'level', 'advertising', 'expenditure', 'different', 'type', 'firm', 'equity', 'risk', 'total', 'systematic', 'unsystematic', 'risk', 'unlike', 'previous', 'empirical', 'finding', 'found', 'increase', 'advertising', 'expenditure', 'significantly', 'increased', 'total', 'unsystematic', 'risk', 'sampled', 'restaurant', 'firm', 'finding', 'provide', 'insight', 'effect', 'advertising', 'firm', 'equity', 'risk', 'literature', 'important', 'theoretical', 'managerial', 'implication', 'restaurant', 'firm', 'copyright', 'taylor', 'francis', 'group', 'advertising', 'equity', 'risk', 'hospitality', 'restaurant', 'systematic', 'risk', 'total', 'risk', 'unsystematic', 'risk', 'stock', 'market', 'reaction', 'unexpected', 'growth', 'marketing', 'expenditure', 'negative', 'sale', 'force', 'contingent', 'spending', 'level', 'advertising', 'firm', 'publicly', 'report', 'marketing', 'expenditure', 'study', 'link', 'firm', 'value', 'marketing', 'consider', 'advertising', 'publicly', 'reported', 'many', 'firm', 'proxy', 'marketing', 'author', 'extend', 'study', 'way', 'first', 'broaden', 'proxy', 'marketing', 'considering', 'advertising', 'sale', 'force', 'second', 'offer', 'explanation', 'fact', 'study', 'linking', 'advertising', 'firm', 'value', 'find', 'positive', 'relationship', 'whereas', 'others', 'find', 'negative', 'relationship', 'accounting', 'literature', 'suggests', 'link', 'firm', 'value', 'unexpected', 'growth', 'sale', 'force', 'expenditure', 'unexpected', 'growth', 'advertising', 'expenditure', 'negative', 'author', 'confirm', 'hypothesized', 'accounting', 'relationship', 'sale', 'force', 'expenditure', 'find', 'contingent', 'relationship', 'advertising', 'expenditure', 'firm', 'value', 'unexpected', 'growth', 'advertising', 'expenditure', 'negatively', 'related', 'firm', 'advertise', 'advertising', 'response', 'threshold', 'positively', 'related', 'firm', 'advertise', 'threshold', 'perhaps', 'contingent', 'relationship', 'difficult', 'analyst', 'learn', 'observation', 'stock', 'market', 'analyst', 'ignore', 'value', 'relevant', 'advertising', 'expenditure', 'information', 'forecast', 'firm', 'value', 'american', 'marketing', 'association', 'advertising', 'analyst', 'forecast', 'cumulative', 'abnormal', 'stock', 'return', 'firm', 'value', 'fundamental', 'signal', 'sale', 'force', 'advertising', 'research', 'development', 'systematic', 'risk', 'firm', 'marketing', 'executive', 'urged', 'speak', 'language', 'finance', 'gain', 'internal', 'support', 'marketing', 'initiative', 'responding', 'call', 'author', 'examine', 'impact', 'firm', 'advertising', 'research', 'development', 'systematic', 'risk', 'stock', 'metric', 'publicly', 'listed', 'firm', 'hypothesize', 'firm', 'advertising', 'expenditure', 'create', 'intangible', 'asset', 'insulate', 'stock', 'market', 'change', 'lowering', 'systematic', 'risk', 'test', 'hypothesis', 'using', 'panel', 'data', 'publicly', 'listed', 'firm', 'consisting', 'five', 'year', 'moving', 'window', 'scale', 'firm', 'advertising', 'expenditure', 'sale', 'controlling', 'factor', 'accounting', 'finance', 'researcher', 'shown', 'associated', 'systematic', 'risk', 'author', 'find', 'advertising', 'sale', 'sale', 'lower', 'firm', 'systematic', 'risk', 'article', 'finding', 'extend', 'prior', 'research', 'focused', 'primarily', 'effect', 'marketing', 'initiative', 'performance', 'metric', 'without', 'consideration', 'systematic', 'risk', 'practice', 'ability', 'advertising', 'reduce', 'systematic', 'risk', 'highlight', 'multifaceted', 'implication', 'advertising', 'research', 'program', 'article', 'finding', 'surprise', 'senior', 'management', 'skeptical', 'financial', 'accountability', 'advertising', 'program', 'american', 'marketing', 'association'], tags=['Kim, MinChung'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import gensim\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:30:09,764 - doc2vec - INFO - collecting all words and their counts\n",
      "2018-12-05 10:30:09,767 - doc2vec - INFO - PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-12-05 10:30:10,323 - doc2vec - INFO - collected 39597 word types and 258 unique tags from a corpus of 258 examples and 1570282 words\n",
      "2018-12-05 10:30:10,324 - word2vec - INFO - Loading a fresh vocabulary\n",
      "2018-12-05 10:30:11,495 - word2vec - INFO - effective_min_count=1 retains 39597 unique words (100% of original 39597, drops 0)\n",
      "2018-12-05 10:30:11,495 - word2vec - INFO - effective_min_count=1 leaves 1570282 word corpus (100% of original 1570282, drops 0)\n",
      "2018-12-05 10:30:11,627 - word2vec - INFO - deleting the raw counts dictionary of 39597 items\n",
      "2018-12-05 10:30:11,628 - word2vec - INFO - sample=0.001 downsamples 21 most-common words\n",
      "2018-12-05 10:30:11,629 - word2vec - INFO - downsampling leaves estimated 1546542 word corpus (98.5% of prior 1570282)\n",
      "2018-12-05 10:30:11,850 - base_any2vec - INFO - estimated required memory for 39597 words and 100 dimensions: 51630900 bytes\n",
      "2018-12-05 10:30:11,851 - word2vec - INFO - resetting layer weights\n",
      "2018-12-05 10:30:12,722 - base_any2vec - INFO - training model with 8 workers on 39597 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-12-05 10:30:13,754 - base_any2vec - INFO - EPOCH 1 - PROGRESS: at 50.00% examples, 612836 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:14,760 - base_any2vec - INFO - EPOCH 1 - PROGRESS: at 94.57% examples, 595245 words/s, in_qsize 7, out_qsize 1\n",
      "2018-12-05 10:30:14,761 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:14,768 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:14,783 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:14,785 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:14,797 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:14,800 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:14,801 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:14,808 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:14,809 - base_any2vec - INFO - EPOCH - 1 : training on 1570282 raw words (1266215 effective words) took 2.1s, 608594 effective words/s\n",
      "2018-12-05 10:30:15,846 - base_any2vec - INFO - EPOCH 2 - PROGRESS: at 54.65% examples, 655982 words/s, in_qsize 13, out_qsize 2\n",
      "2018-12-05 10:30:16,755 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:16,758 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:16,764 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:16,773 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:16,788 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:16,795 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:16,805 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:16,812 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:16,813 - base_any2vec - INFO - EPOCH - 2 : training on 1570282 raw words (1266287 effective words) took 2.0s, 635676 effective words/s\n",
      "2018-12-05 10:30:17,828 - base_any2vec - INFO - EPOCH 3 - PROGRESS: at 45.74% examples, 574084 words/s, in_qsize 16, out_qsize 0\n",
      "2018-12-05 10:30:18,743 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:18,779 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:18,799 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:18,820 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:18,826 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:18,842 - base_any2vec - INFO - EPOCH 3 - PROGRESS: at 98.84% examples, 617766 words/s, in_qsize 2, out_qsize 1\n",
      "2018-12-05 10:30:18,844 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:18,855 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:18,858 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:18,859 - base_any2vec - INFO - EPOCH - 3 : training on 1570282 raw words (1266302 effective words) took 2.0s, 621555 effective words/s\n",
      "2018-12-05 10:30:19,875 - base_any2vec - INFO - EPOCH 4 - PROGRESS: at 54.26% examples, 661872 words/s, in_qsize 13, out_qsize 2\n",
      "2018-12-05 10:30:20,664 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:20,668 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:20,673 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:20,675 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:20,691 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:20,704 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:20,724 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:20,729 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:20,730 - base_any2vec - INFO - EPOCH - 4 : training on 1570282 raw words (1266209 effective words) took 1.9s, 679284 effective words/s\n",
      "2018-12-05 10:30:21,738 - base_any2vec - INFO - EPOCH 5 - PROGRESS: at 40.31% examples, 513497 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:22,743 - base_any2vec - INFO - EPOCH 5 - PROGRESS: at 89.53% examples, 571706 words/s, in_qsize 14, out_qsize 0\n",
      "2018-12-05 10:30:22,848 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:22,858 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:22,862 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:22,872 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:22,887 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:22,891 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:22,893 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:22,899 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:22,900 - base_any2vec - INFO - EPOCH - 5 : training on 1570282 raw words (1266202 effective words) took 2.2s, 585510 effective words/s\n",
      "2018-12-05 10:30:23,915 - base_any2vec - INFO - EPOCH 6 - PROGRESS: at 43.80% examples, 557728 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:24,938 - base_any2vec - INFO - EPOCH 6 - PROGRESS: at 89.92% examples, 564929 words/s, in_qsize 14, out_qsize 0\n",
      "2018-12-05 10:30:25,064 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:25,080 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:25,107 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:25,112 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:25,126 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:25,126 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:30:25,136 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:25,148 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:25,152 - base_any2vec - INFO - EPOCH - 6 : training on 1570282 raw words (1266148 effective words) took 2.2s, 564554 effective words/s\n",
      "2018-12-05 10:30:26,168 - base_any2vec - INFO - EPOCH 7 - PROGRESS: at 58.91% examples, 710602 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:26,817 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:26,831 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:26,832 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:26,843 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:26,845 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:26,858 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:26,860 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:26,867 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:26,868 - base_any2vec - INFO - EPOCH - 7 : training on 1570282 raw words (1266205 effective words) took 1.7s, 743458 effective words/s\n",
      "2018-12-05 10:30:27,912 - base_any2vec - INFO - EPOCH 8 - PROGRESS: at 38.37% examples, 470293 words/s, in_qsize 16, out_qsize 0\n",
      "2018-12-05 10:30:28,914 - base_any2vec - INFO - EPOCH 8 - PROGRESS: at 83.33% examples, 523629 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:29,102 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:29,112 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:29,119 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:29,120 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:29,121 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:29,135 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:29,138 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:29,152 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:29,153 - base_any2vec - INFO - EPOCH - 8 : training on 1570282 raw words (1266412 effective words) took 2.3s, 556410 effective words/s\n",
      "2018-12-05 10:30:30,173 - base_any2vec - INFO - EPOCH 9 - PROGRESS: at 37.98% examples, 478745 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:31,202 - base_any2vec - INFO - EPOCH 9 - PROGRESS: at 84.50% examples, 533146 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:31,374 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:31,380 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:31,382 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:31,402 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:31,404 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:31,411 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:31,418 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:31,418 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:31,419 - base_any2vec - INFO - EPOCH - 9 : training on 1570282 raw words (1266175 effective words) took 2.3s, 560248 effective words/s\n",
      "2018-12-05 10:30:32,433 - base_any2vec - INFO - EPOCH 10 - PROGRESS: at 49.22% examples, 616563 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:33,319 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:33,320 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:33,326 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:33,335 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:33,336 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:33,337 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:33,339 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:33,353 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:33,354 - base_any2vec - INFO - EPOCH - 10 : training on 1570282 raw words (1266176 effective words) took 1.9s, 656529 effective words/s\n",
      "2018-12-05 10:30:34,386 - base_any2vec - INFO - EPOCH 11 - PROGRESS: at 53.49% examples, 638289 words/s, in_qsize 13, out_qsize 2\n",
      "2018-12-05 10:30:35,285 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:35,293 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:35,305 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:35,307 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:35,308 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:35,314 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:35,329 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:35,331 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:35,333 - base_any2vec - INFO - EPOCH - 11 : training on 1570282 raw words (1266305 effective words) took 2.0s, 641712 effective words/s\n",
      "2018-12-05 10:30:36,342 - base_any2vec - INFO - EPOCH 12 - PROGRESS: at 60.08% examples, 719173 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:37,030 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:37,053 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:37,060 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:37,074 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:37,075 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:37,087 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:37,088 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:37,099 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:37,100 - base_any2vec - INFO - EPOCH - 12 : training on 1570282 raw words (1266260 effective words) took 1.8s, 718922 effective words/s\n",
      "2018-12-05 10:30:38,121 - base_any2vec - INFO - EPOCH 13 - PROGRESS: at 52.33% examples, 630814 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:39,139 - base_any2vec - INFO - EPOCH 13 - PROGRESS: at 91.09% examples, 562660 words/s, in_qsize 14, out_qsize 0\n",
      "2018-12-05 10:30:39,250 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:39,275 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:39,323 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:39,331 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:39,351 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:39,379 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:30:39,387 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:39,389 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:39,390 - base_any2vec - INFO - EPOCH - 13 : training on 1570282 raw words (1266099 effective words) took 2.3s, 554054 effective words/s\n",
      "2018-12-05 10:30:40,406 - base_any2vec - INFO - EPOCH 14 - PROGRESS: at 38.37% examples, 486519 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:41,431 - base_any2vec - INFO - EPOCH 14 - PROGRESS: at 84.50% examples, 534279 words/s, in_qsize 13, out_qsize 2\n",
      "2018-12-05 10:30:41,680 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:41,682 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:41,683 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:41,692 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:41,696 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:41,699 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:41,708 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:41,715 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:41,716 - base_any2vec - INFO - EPOCH - 14 : training on 1570282 raw words (1266267 effective words) took 2.3s, 547985 effective words/s\n",
      "2018-12-05 10:30:42,748 - base_any2vec - INFO - EPOCH 15 - PROGRESS: at 39.15% examples, 490600 words/s, in_qsize 16, out_qsize 0\n",
      "2018-12-05 10:30:43,761 - base_any2vec - INFO - EPOCH 15 - PROGRESS: at 91.47% examples, 572089 words/s, in_qsize 12, out_qsize 0\n",
      "2018-12-05 10:30:43,870 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:43,874 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:43,881 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:43,882 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:43,883 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:43,894 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:43,906 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:43,918 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:43,919 - base_any2vec - INFO - EPOCH - 15 : training on 1570282 raw words (1266216 effective words) took 2.2s, 577429 effective words/s\n",
      "2018-12-05 10:30:44,943 - base_any2vec - INFO - EPOCH 16 - PROGRESS: at 55.81% examples, 670423 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:45,875 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:45,877 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:45,895 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:45,904 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:45,907 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:45,912 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:45,920 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:45,927 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:45,928 - base_any2vec - INFO - EPOCH - 16 : training on 1570282 raw words (1266133 effective words) took 2.0s, 631960 effective words/s\n",
      "2018-12-05 10:30:46,937 - base_any2vec - INFO - EPOCH 17 - PROGRESS: at 61.24% examples, 737177 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:47,766 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:47,771 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:47,775 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:47,780 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:47,792 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:47,812 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:47,828 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:47,835 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:47,836 - base_any2vec - INFO - EPOCH - 17 : training on 1570282 raw words (1266283 effective words) took 1.9s, 665550 effective words/s\n",
      "2018-12-05 10:30:48,849 - base_any2vec - INFO - EPOCH 18 - PROGRESS: at 50.00% examples, 623080 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:49,679 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:49,685 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:49,710 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:49,719 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:49,725 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:49,730 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:49,738 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:49,740 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:49,742 - base_any2vec - INFO - EPOCH - 18 : training on 1570282 raw words (1266267 effective words) took 1.9s, 665624 effective words/s\n",
      "2018-12-05 10:30:50,763 - base_any2vec - INFO - EPOCH 19 - PROGRESS: at 43.80% examples, 553192 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:51,689 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:51,701 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:51,711 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:51,718 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:51,724 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:51,734 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:51,735 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:51,741 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:51,742 - base_any2vec - INFO - EPOCH - 19 : training on 1570282 raw words (1266222 effective words) took 2.0s, 635420 effective words/s\n",
      "2018-12-05 10:30:52,750 - base_any2vec - INFO - EPOCH 20 - PROGRESS: at 61.63% examples, 748501 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:53,380 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:53,384 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:53,387 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:53,407 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:53,417 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:53,438 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:53,452 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:30:53,453 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:53,454 - base_any2vec - INFO - EPOCH - 20 : training on 1570282 raw words (1266433 effective words) took 1.7s, 742158 effective words/s\n",
      "2018-12-05 10:30:53,455 - base_any2vec - INFO - training on a 31405640 raw words (25324816 effective words) took 40.7s, 621784 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Doc2Vec(vector_size=100, window=10, min_count=1, workers=8, alpha=0.025, min_alpha=0.015, \n",
    "                              epochs=20)\n",
    "\n",
    "#sample=1e-4, negative=5,\n",
    "\n",
    "#shuffling is better (ot needed at each trianing epoch\n",
    "shuffle(tagged_data)\n",
    "#Build vocabulary from a sequence of sentences \n",
    "model.build_vocab(tagged_data)\n",
    "#Update the model’s neural weights from a sequence of sentences\n",
    "model.train(tagged_data, epochs=model.epochs, total_examples=model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"finalmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametertunning(v):\n",
    "    model = gensim.models.Doc2Vec(vector_size=v, window=10, min_count=1, workers=8, alpha=0.025, min_alpha=0.015, \n",
    "                              epochs=20)\n",
    "    profname = list(model.docvecs.doctags.keys())\n",
    "    final = []\n",
    "    for j in profname:\n",
    "        count = 0\n",
    "        mostsimilar = model.docvecs.most_similar(j)\n",
    "        for a,b in mostsimilar:\n",
    "            if a.split('/')[1] == i.split('/')[1]:\n",
    "                count += 1\n",
    "            else:\n",
    "                pass\n",
    "        final.append(count)\n",
    "        average = sum(final)/len(final)\n",
    "        return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
