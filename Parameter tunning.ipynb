{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/seungheondoh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Author Keywords</th>\n",
       "      <th>Name</th>\n",
       "      <th>ResearchInterest</th>\n",
       "      <th>Title</th>\n",
       "      <th>_id</th>\n",
       "      <th>department</th>\n",
       "      <th>docs</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Entrepreneurs' decisions to exploit opportuni...</td>\n",
       "      <td>[nan, Behavioral decision-making; Business gro...</td>\n",
       "      <td>Choi, Young Rok(최영록)</td>\n",
       "      <td>Entrepreneurship, Innovation Strategy, Family ...</td>\n",
       "      <td>[Entrepreneurs' decisions to exploit opportuni...</td>\n",
       "      <td>5bf7bdc420c66c03397b5abd</td>\n",
       "      <td>Graduate School of Technology and Innovation M...</td>\n",
       "      <td>[[Entrepreneurs' decisions to exploit opportun...</td>\n",
       "      <td>Entrepreneurs' decisions to exploit opportuni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Organizations use project teams to lower sear...</td>\n",
       "      <td>[expertise awareness; expertise use; knowledge...</td>\n",
       "      <td>Hong, Woonki(홍운기)</td>\n",
       "      <td>Expertise utilization, Transactive menory syst...</td>\n",
       "      <td>[Explaining dyadic expertise use in knowledge ...</td>\n",
       "      <td>5bf7bdc520c66c03397b5abe</td>\n",
       "      <td>School of Business Administration(경영학부)</td>\n",
       "      <td>[[Explaining dyadic expertise use in knowledge...</td>\n",
       "      <td>Explaining dyadic expertise use in knowledge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[This paper investigates information leakage f...</td>\n",
       "      <td>[nan, nan, Business group; Chaebol; Control-ca...</td>\n",
       "      <td>Jung, Kooyul(정구열)</td>\n",
       "      <td>Accounting Information and capital market, Ana...</td>\n",
       "      <td>[Trading behavior prior to public release of a...</td>\n",
       "      <td>5bf7bdc520c66c03397b5abf</td>\n",
       "      <td>Graduate School of Technology and Innovation M...</td>\n",
       "      <td>[[Trading behavior prior to public release of ...</td>\n",
       "      <td>Trading behavior prior to public release of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Studies on the effects of corporate social re...</td>\n",
       "      <td>[annual reports; communication; corporate soci...</td>\n",
       "      <td>Kim, MinChung(김민중)</td>\n",
       "      <td>Marketing strategies and their impact on finan...</td>\n",
       "      <td>[CSR and Shareholder Value in the Restaurant I...</td>\n",
       "      <td>5bf7bdc520c66c03397b5ac0</td>\n",
       "      <td>School of Business Administration(경영학부)</td>\n",
       "      <td>[[CSR and Shareholder Value in the Restaurant ...</td>\n",
       "      <td>CSR and Shareholder Value in the Restaurant I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Brand communities and corporate social respon...</td>\n",
       "      <td>[Brand community; Consumer communities; Corpor...</td>\n",
       "      <td>Kim, Molan(김모란)</td>\n",
       "      <td>Online Database Marketing, Social Media Market...</td>\n",
       "      <td>[Consumer Communities Do Well, But Will They D...</td>\n",
       "      <td>5bf7bdc520c66c03397b5ac1</td>\n",
       "      <td>School of Business Administration(경영학부)</td>\n",
       "      <td>[[Consumer Communities Do Well, But Will They ...</td>\n",
       "      <td>Consumer Communities Do Well, But Will They D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  \\\n",
       "0  [Entrepreneurs' decisions to exploit opportuni...   \n",
       "1  [Organizations use project teams to lower sear...   \n",
       "2  [This paper investigates information leakage f...   \n",
       "3  [Studies on the effects of corporate social re...   \n",
       "4  [Brand communities and corporate social respon...   \n",
       "\n",
       "                                     Author Keywords                  Name  \\\n",
       "0  [nan, Behavioral decision-making; Business gro...  Choi, Young Rok(최영록)   \n",
       "1  [expertise awareness; expertise use; knowledge...     Hong, Woonki(홍운기)   \n",
       "2  [nan, nan, Business group; Chaebol; Control-ca...     Jung, Kooyul(정구열)   \n",
       "3  [annual reports; communication; corporate soci...    Kim, MinChung(김민중)   \n",
       "4  [Brand community; Consumer communities; Corpor...       Kim, Molan(김모란)   \n",
       "\n",
       "                                    ResearchInterest  \\\n",
       "0  Entrepreneurship, Innovation Strategy, Family ...   \n",
       "1  Expertise utilization, Transactive menory syst...   \n",
       "2  Accounting Information and capital market, Ana...   \n",
       "3  Marketing strategies and their impact on finan...   \n",
       "4  Online Database Marketing, Social Media Market...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  [Entrepreneurs' decisions to exploit opportuni...   \n",
       "1  [Explaining dyadic expertise use in knowledge ...   \n",
       "2  [Trading behavior prior to public release of a...   \n",
       "3  [CSR and Shareholder Value in the Restaurant I...   \n",
       "4  [Consumer Communities Do Well, But Will They D...   \n",
       "\n",
       "                        _id  \\\n",
       "0  5bf7bdc420c66c03397b5abd   \n",
       "1  5bf7bdc520c66c03397b5abe   \n",
       "2  5bf7bdc520c66c03397b5abf   \n",
       "3  5bf7bdc520c66c03397b5ac0   \n",
       "4  5bf7bdc520c66c03397b5ac1   \n",
       "\n",
       "                                          department  \\\n",
       "0  Graduate School of Technology and Innovation M...   \n",
       "1            School of Business Administration(경영학부)   \n",
       "2  Graduate School of Technology and Innovation M...   \n",
       "3            School of Business Administration(경영학부)   \n",
       "4            School of Business Administration(경영학부)   \n",
       "\n",
       "                                                docs  \\\n",
       "0  [[Entrepreneurs' decisions to exploit opportun...   \n",
       "1  [[Explaining dyadic expertise use in knowledge...   \n",
       "2  [[Trading behavior prior to public release of ...   \n",
       "3  [[CSR and Shareholder Value in the Restaurant ...   \n",
       "4  [[Consumer Communities Do Well, But Will They ...   \n",
       "\n",
       "                                                text  \n",
       "0   Entrepreneurs' decisions to exploit opportuni...  \n",
       "1   Explaining dyadic expertise use in knowledge ...  \n",
       "2   Trading behavior prior to public release of a...  \n",
       "3   CSR and Shareholder Value in the Restaurant I...  \n",
       "4   Consumer Communities Do Well, But Will They D...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "db = client.aboutdepart\n",
    "database = list(db.Docdb.find())\n",
    "testdf = pd.DataFrame(database)\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.to_csv('prof2vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "\n",
    "    docs = docs.lower()  # Convert to lowercase.\n",
    "    docs = tokenizer.tokenize(docs)  # Split into words.\n",
    "    docs = [w for w in docs if not w in stopwords.words('english')]\n",
    "    \n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [token for token in docs if not token.isdigit()]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [token for token in docs if len(token) > 3]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [lemmatizer.lemmatize(token) for token in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(testdf['text']):\n",
    "    add = docs_preprocessor(i)\n",
    "    testdf.iloc[idx]['text'] = add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(testdf['Name']):\n",
    "    testdf.iloc[idx]['Name'] = i.split(\"(\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdata = zip(testdf['Name'],testdf['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in zipdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['shareholder', 'value', 'restaurant', 'industry', 'role', 'communication', 'annual', 'report', 'study', 'effect', 'corporate', 'social', 'responsibility', 'shareholder', 'value', 'identified', 'communication', 'marketing', 'channel', 'mechanism', 'necessary', 'translate', 'shareholder', 'value', 'study', 'annual', 'report', 'considered', 'primary', 'information', 'source', 'used', 'firm', 'financial', 'stakeholder', 'investor', 'analyst', 'valuation', 'firm', 'specifically', 'study', 'examined', 'whether', 'extent', 'restaurant', 'firm', 'communicate', 'annual', 'report', 'influence', 'effect', 'shareholder', 'value', 'return', 'tobin', 'risk', 'equity', 'holder', 'risk', 'disaggregated', 'distinct', 'type', 'primary', 'nonprimary', 'stakeholder', 'result', 'corroborate', 'communicating', 'nonprimary', 'stakeholder', 'annual', 'report', 'help', 'nonprimary', 'stakeholder', 'increase', 'shareholder', 'value', 'reducing', 'equity', 'holder', 'risk', 'whereas', 'communicating', 'primary', 'stakeholder', 'affect', 'value', 'relevance', 'primary', 'stakeholder', 'change', 'either', 'tobin', 'equity', 'holder', 'risk', 'author', 'annual', 'report', 'communication', 'corporate', 'social', 'responsibility', 'equity', 'holder', 'risk', 'tobin', 'effect', 'corporate', 'social', 'responsibility', 'corporate', 'financial', 'performance', 'competitive', 'action', 'perspective', 'attempt', 'provide', 'nuanced', 'view', 'relationship', 'corporate', 'social', 'responsibility', 'firm', 'financial', 'performance', 'using', 'competitive', 'action', 'perspective', 'argue', 'competitive', 'action', 'considered', 'important', 'contingency', 'determines', 'effect', 'activity', 'firm', 'financial', 'performance', 'using', 'data', 'publicly', 'listed', 'firm', 'software', 'industry', 'found', 'socially', 'responsible', 'activity', 'positive', 'enhance', 'firm', 'financial', 'performance', 'firm', 'competitive', 'action', 'level', 'high', 'whereas', 'socially', 'irresponsible', 'activity', 'negative', 'actually', 'improve', 'firm', 'financial', 'performance', 'competitive', 'action', 'level', 'introducing', 'competitive', 'action', 'important', 'contingency', 'study', 'contributes', 'literature', 'strategic', 'management', 'author', 'competitive', 'action', 'financial', 'performance', 'negative', 'positive', 'pride', 'lead', 'corporate', 'managerial', 'hubris', 'strategic', 'emphasis', 'firm', 'strategic', 'emphasis', 'value', 'creation', 'versus', 'appropriation', 'typically', 'reflected', 'resource', 'allocation', 'advertising', 'central', 'corporate', 'decision', 'significantly', 'influence', 'financial', 'performance', 'however', 'driver', 'decision', 'remain', 'underexplored', 'study', 'identifies', 'significant', 'predictor', 'strategic', 'emphasis', 'namely', 'corporate', 'managerial', 'hubris', 'reveals', 'boundary', 'condition', 'leveraging', 'unique', 'dataset', 'based', 'text', 'mining', 'press', 'release', 'issued', 'firm', 'across', 'year', 'author', 'demonstrate', 'high', 'corporate', 'managerial', 'hubris', 'predicts', 'strategic', 'emphasis', 'advertising', 'relative', 'however', 'effect', 'mitigated', 'significantly', 'firm', 'maturity', 'corporate', 'governance', 'industry', 'level', 'strategic', 'emphasis', 'result', 'provide', 'novel', 'insight', 'effect', 'hubris', 'firm', 'spending', 'situation', 'wherein', 'marketing', 'decision', 'tend', 'subject', 'manager', 'psychological', 'bias', 'mean', 'preventing', 'investment', 'marketing', 'strategy', 'recruitment', 'training', 'manager', 'academy', 'marketing', 'science', 'advertising', 'corporate', 'managerial', 'hubris', 'strategic', 'emphasis', 'corporate', 'social', 'responsibility', 'equity', 'holder', 'risk', 'hospitality', 'industry', 'study', 'examined', 'whether', 'corporate', 'social', 'responsibility', 'enhances', 'firm', 'value', 'shareholder', 'ultimately', 'fund', 'firm', 'initiative', 'specifically', 'investigated', 'relationship', 'activity', 'hospitality', 'firm', 'risk', 'associated', 'equity', 'holding', 'firm', 'using', 'msci', 'environmental', 'social', 'governance', 'rating', 'measured', 'extent', 'effort', 'firm', 'tested', 'effect', 'different', 'type', 'equity', 'holder', 'risk', 'systematic', 'unsystematic', 'risk', 'across', 'four', 'segment', 'hospitality', 'industry', 'airline', 'hotel', 'casino', 'restaurant', 'found', 'reduce', 'systematic', 'risk', 'restaurant', 'casino', 'firm', 'significantly', 'whereas', 'significant', 'influence', 'unsystematic', 'risk', 'segment', 'result', 'study', 'important', 'theoretical', 'practical', 'implication', 'academia', 'hospitality', 'industry', 'author', 'airline', 'casino', 'corporate', 'social', 'responsibility', 'equity', 'risk', 'hotel', 'restaurant', 'tourism', 'equity', 'incentive', 'shareholder', 'value', 'moderating', 'role', 'managerial', 'discretion', 'chief', 'marketing', 'officer', 'cmos', 'marketing', 'leader', 'respective', 'firm', 'responsible', 'creating', 'shareholder', 'value', 'incentivize', 'cmos', 'focus', 'shareholder', 'interest', 'increasing', 'number', 'firm', 'compensating', 'cmos', 'equity', 'study', 'examines', 'impact', 'equity', 'incentive', 'shareholder', 'value', 'introduces', 'three', 'different', 'form', 'managerial', 'discretion', 'given', 'cmos', 'contingency', 'determining', 'impact', 'equity', 'incentive', 'le', 'result', 'based', 'compensation', 'data', 'confirm', 'value', 'relevance', 'equity', 'incentive', 'beyond', 'equity', 'incentive', 'allocated', 'member', 'furthermore', 'result', 'reveal', 'support', 'moderating', 'effect', 'financial', 'strategic', 'operational', 'form', 'managerial', 'discretion', 'theoretically', 'finding', 'identify', 'hitherto', 'unrecognized', 'boundary', 'condition', 'determining', 'impact', 'equity', 'shareholder', 'value', 'higher', 'lower', 'address', 'several', 'limitation', 'characterizing', 'past', 'research', 'topic', 'managerially', 'result', 'guide', 'firm', 'structuring', 'compensation', 'discretion', 'way', 'optimize', 'effect', 'cmos', 'shareholder', 'value', 'elsevier', 'agency', 'problem', 'chief', 'marketing', 'officer', 'equity', 'incentive', 'equity', 'based', 'compensation', 'firm', 'value', 'managerial', 'discretion', 'corporate', 'social', 'responsibility', 'shareholder', 'value', 'restaurant', 'firm', 'response', 'recent', 'call', 'understanding', 'corporate', 'social', 'responsibility', 'influence', 'shareholder', 'value', 'study', 'examined', 'effect', 'strengthening', 'strengthening', 'firm', 'reputation', 'concerning', 'potentially', 'diminishing', 'reputation', 'activity', 'publicly', 'listed', 'restaurant', 'firm', 'shareholder', 'value', 'unlike', 'previous', 'study', 'hospitality', 'literature', 'focused', 'level', 'stock', 'return', 'study', 'considered', 'systematic', 'risk', 'another', 'important', 'determinant', 'shareholder', 'value', 'study', 'tested', 'whether', 'strengthening', 'concerning', 'action', 'restaurant', 'firm', 'associated', 'shareholder', 'value', 'based', 'systematic', 'risk', 'tobin', 'strengthening', 'action', 'found', 'enhance', 'shareholder', 'value', 'increasing', 'tobin', 'whereas', 'weakening', 'action', 'reduce', 'shareholder', 'value', 'increasing', 'systematic', 'risk', 'firm', 'analyzing', 'different', 'effect', 'strengthening', 'concerning', 'action', 'study', 'provides', 'interesting', 'important', 'implication', 'hospitality', 'literature', 'practitioner', 'restaurant', 'firm', 'elsevier', 'corporate', 'social', 'responsibility', 'hospitality', 'restaurant', 'shareholder', 'value', 'stock', 'return', 'systematic', 'risk', 'advertising', 'firm', 'risk', 'study', 'restaurant', 'industry', 'incorporating', 'recent', 'call', 'understanding', 'firm', 'equity', 'risk', 'relation', 'firm', 'marketing', 'effort', 'study', 'examined', 'impact', 'firm', 'level', 'advertising', 'spending', 'firm', 'equity', 'risk', 'publicly', 'listed', 'firm', 'restaurant', 'industry', 'hospitality', 'industry', 'study', 'hypothesized', 'tested', 'effect', 'firm', 'level', 'advertising', 'expenditure', 'different', 'type', 'firm', 'equity', 'risk', 'total', 'systematic', 'unsystematic', 'risk', 'unlike', 'previous', 'empirical', 'finding', 'found', 'increase', 'advertising', 'expenditure', 'significantly', 'increased', 'total', 'unsystematic', 'risk', 'sampled', 'restaurant', 'firm', 'finding', 'provide', 'insight', 'effect', 'advertising', 'firm', 'equity', 'risk', 'literature', 'important', 'theoretical', 'managerial', 'implication', 'restaurant', 'firm', 'copyright', 'taylor', 'francis', 'group', 'advertising', 'equity', 'risk', 'hospitality', 'restaurant', 'systematic', 'risk', 'total', 'risk', 'unsystematic', 'risk', 'stock', 'market', 'reaction', 'unexpected', 'growth', 'marketing', 'expenditure', 'negative', 'sale', 'force', 'contingent', 'spending', 'level', 'advertising', 'firm', 'publicly', 'report', 'marketing', 'expenditure', 'study', 'link', 'firm', 'value', 'marketing', 'consider', 'advertising', 'publicly', 'reported', 'many', 'firm', 'proxy', 'marketing', 'author', 'extend', 'study', 'way', 'first', 'broaden', 'proxy', 'marketing', 'considering', 'advertising', 'sale', 'force', 'second', 'offer', 'explanation', 'fact', 'study', 'linking', 'advertising', 'firm', 'value', 'find', 'positive', 'relationship', 'whereas', 'others', 'find', 'negative', 'relationship', 'accounting', 'literature', 'suggests', 'link', 'firm', 'value', 'unexpected', 'growth', 'sale', 'force', 'expenditure', 'unexpected', 'growth', 'advertising', 'expenditure', 'negative', 'author', 'confirm', 'hypothesized', 'accounting', 'relationship', 'sale', 'force', 'expenditure', 'find', 'contingent', 'relationship', 'advertising', 'expenditure', 'firm', 'value', 'unexpected', 'growth', 'advertising', 'expenditure', 'negatively', 'related', 'firm', 'advertise', 'advertising', 'response', 'threshold', 'positively', 'related', 'firm', 'advertise', 'threshold', 'perhaps', 'contingent', 'relationship', 'difficult', 'analyst', 'learn', 'observation', 'stock', 'market', 'analyst', 'ignore', 'value', 'relevant', 'advertising', 'expenditure', 'information', 'forecast', 'firm', 'value', 'american', 'marketing', 'association', 'advertising', 'analyst', 'forecast', 'cumulative', 'abnormal', 'stock', 'return', 'firm', 'value', 'fundamental', 'signal', 'sale', 'force', 'advertising', 'research', 'development', 'systematic', 'risk', 'firm', 'marketing', 'executive', 'urged', 'speak', 'language', 'finance', 'gain', 'internal', 'support', 'marketing', 'initiative', 'responding', 'call', 'author', 'examine', 'impact', 'firm', 'advertising', 'research', 'development', 'systematic', 'risk', 'stock', 'metric', 'publicly', 'listed', 'firm', 'hypothesize', 'firm', 'advertising', 'expenditure', 'create', 'intangible', 'asset', 'insulate', 'stock', 'market', 'change', 'lowering', 'systematic', 'risk', 'test', 'hypothesis', 'using', 'panel', 'data', 'publicly', 'listed', 'firm', 'consisting', 'five', 'year', 'moving', 'window', 'scale', 'firm', 'advertising', 'expenditure', 'sale', 'controlling', 'factor', 'accounting', 'finance', 'researcher', 'shown', 'associated', 'systematic', 'risk', 'author', 'find', 'advertising', 'sale', 'sale', 'lower', 'firm', 'systematic', 'risk', 'article', 'finding', 'extend', 'prior', 'research', 'focused', 'primarily', 'effect', 'marketing', 'initiative', 'performance', 'metric', 'without', 'consideration', 'systematic', 'risk', 'practice', 'ability', 'advertising', 'reduce', 'systematic', 'risk', 'highlight', 'multifaceted', 'implication', 'advertising', 'research', 'program', 'article', 'finding', 'surprise', 'senior', 'management', 'skeptical', 'financial', 'accountability', 'advertising', 'program', 'american', 'marketing', 'association'], tags=['Kim, MinChung'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import gensim\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:30:09,764 - doc2vec - INFO - collecting all words and their counts\n",
      "2018-12-05 10:30:09,767 - doc2vec - INFO - PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-12-05 10:30:10,323 - doc2vec - INFO - collected 39597 word types and 258 unique tags from a corpus of 258 examples and 1570282 words\n",
      "2018-12-05 10:30:10,324 - word2vec - INFO - Loading a fresh vocabulary\n",
      "2018-12-05 10:30:11,495 - word2vec - INFO - effective_min_count=1 retains 39597 unique words (100% of original 39597, drops 0)\n",
      "2018-12-05 10:30:11,495 - word2vec - INFO - effective_min_count=1 leaves 1570282 word corpus (100% of original 1570282, drops 0)\n",
      "2018-12-05 10:30:11,627 - word2vec - INFO - deleting the raw counts dictionary of 39597 items\n",
      "2018-12-05 10:30:11,628 - word2vec - INFO - sample=0.001 downsamples 21 most-common words\n",
      "2018-12-05 10:30:11,629 - word2vec - INFO - downsampling leaves estimated 1546542 word corpus (98.5% of prior 1570282)\n",
      "2018-12-05 10:30:11,850 - base_any2vec - INFO - estimated required memory for 39597 words and 100 dimensions: 51630900 bytes\n",
      "2018-12-05 10:30:11,851 - word2vec - INFO - resetting layer weights\n",
      "2018-12-05 10:30:12,722 - base_any2vec - INFO - training model with 8 workers on 39597 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-12-05 10:30:13,754 - base_any2vec - INFO - EPOCH 1 - PROGRESS: at 50.00% examples, 612836 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:14,760 - base_any2vec - INFO - EPOCH 1 - PROGRESS: at 94.57% examples, 595245 words/s, in_qsize 7, out_qsize 1\n",
      "2018-12-05 10:30:14,761 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:14,768 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:14,783 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:14,785 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:14,797 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:14,800 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:14,801 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:14,808 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:14,809 - base_any2vec - INFO - EPOCH - 1 : training on 1570282 raw words (1266215 effective words) took 2.1s, 608594 effective words/s\n",
      "2018-12-05 10:30:15,846 - base_any2vec - INFO - EPOCH 2 - PROGRESS: at 54.65% examples, 655982 words/s, in_qsize 13, out_qsize 2\n",
      "2018-12-05 10:30:16,755 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:16,758 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:16,764 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:16,773 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:16,788 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:16,795 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:16,805 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:16,812 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:16,813 - base_any2vec - INFO - EPOCH - 2 : training on 1570282 raw words (1266287 effective words) took 2.0s, 635676 effective words/s\n",
      "2018-12-05 10:30:17,828 - base_any2vec - INFO - EPOCH 3 - PROGRESS: at 45.74% examples, 574084 words/s, in_qsize 16, out_qsize 0\n",
      "2018-12-05 10:30:18,743 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:18,779 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:18,799 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:18,820 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:18,826 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:18,842 - base_any2vec - INFO - EPOCH 3 - PROGRESS: at 98.84% examples, 617766 words/s, in_qsize 2, out_qsize 1\n",
      "2018-12-05 10:30:18,844 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:18,855 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:18,858 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:18,859 - base_any2vec - INFO - EPOCH - 3 : training on 1570282 raw words (1266302 effective words) took 2.0s, 621555 effective words/s\n",
      "2018-12-05 10:30:19,875 - base_any2vec - INFO - EPOCH 4 - PROGRESS: at 54.26% examples, 661872 words/s, in_qsize 13, out_qsize 2\n",
      "2018-12-05 10:30:20,664 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:20,668 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:20,673 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:20,675 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:20,691 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:20,704 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:20,724 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:20,729 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:20,730 - base_any2vec - INFO - EPOCH - 4 : training on 1570282 raw words (1266209 effective words) took 1.9s, 679284 effective words/s\n",
      "2018-12-05 10:30:21,738 - base_any2vec - INFO - EPOCH 5 - PROGRESS: at 40.31% examples, 513497 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:22,743 - base_any2vec - INFO - EPOCH 5 - PROGRESS: at 89.53% examples, 571706 words/s, in_qsize 14, out_qsize 0\n",
      "2018-12-05 10:30:22,848 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:22,858 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:22,862 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:22,872 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:22,887 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:22,891 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:22,893 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:22,899 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:22,900 - base_any2vec - INFO - EPOCH - 5 : training on 1570282 raw words (1266202 effective words) took 2.2s, 585510 effective words/s\n",
      "2018-12-05 10:30:23,915 - base_any2vec - INFO - EPOCH 6 - PROGRESS: at 43.80% examples, 557728 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:24,938 - base_any2vec - INFO - EPOCH 6 - PROGRESS: at 89.92% examples, 564929 words/s, in_qsize 14, out_qsize 0\n",
      "2018-12-05 10:30:25,064 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:25,080 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:25,107 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:25,112 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:25,126 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:25,126 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:30:25,136 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:25,148 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:25,152 - base_any2vec - INFO - EPOCH - 6 : training on 1570282 raw words (1266148 effective words) took 2.2s, 564554 effective words/s\n",
      "2018-12-05 10:30:26,168 - base_any2vec - INFO - EPOCH 7 - PROGRESS: at 58.91% examples, 710602 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:26,817 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:26,831 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:26,832 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:26,843 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:26,845 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:26,858 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:26,860 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:26,867 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:26,868 - base_any2vec - INFO - EPOCH - 7 : training on 1570282 raw words (1266205 effective words) took 1.7s, 743458 effective words/s\n",
      "2018-12-05 10:30:27,912 - base_any2vec - INFO - EPOCH 8 - PROGRESS: at 38.37% examples, 470293 words/s, in_qsize 16, out_qsize 0\n",
      "2018-12-05 10:30:28,914 - base_any2vec - INFO - EPOCH 8 - PROGRESS: at 83.33% examples, 523629 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:29,102 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:29,112 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:29,119 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:29,120 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:29,121 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:29,135 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:29,138 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:29,152 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:29,153 - base_any2vec - INFO - EPOCH - 8 : training on 1570282 raw words (1266412 effective words) took 2.3s, 556410 effective words/s\n",
      "2018-12-05 10:30:30,173 - base_any2vec - INFO - EPOCH 9 - PROGRESS: at 37.98% examples, 478745 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:31,202 - base_any2vec - INFO - EPOCH 9 - PROGRESS: at 84.50% examples, 533146 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:31,374 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:31,380 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:31,382 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:31,402 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:31,404 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:31,411 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:31,418 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:31,418 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:31,419 - base_any2vec - INFO - EPOCH - 9 : training on 1570282 raw words (1266175 effective words) took 2.3s, 560248 effective words/s\n",
      "2018-12-05 10:30:32,433 - base_any2vec - INFO - EPOCH 10 - PROGRESS: at 49.22% examples, 616563 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:33,319 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:33,320 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:33,326 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:33,335 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:33,336 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:33,337 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:33,339 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:33,353 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:33,354 - base_any2vec - INFO - EPOCH - 10 : training on 1570282 raw words (1266176 effective words) took 1.9s, 656529 effective words/s\n",
      "2018-12-05 10:30:34,386 - base_any2vec - INFO - EPOCH 11 - PROGRESS: at 53.49% examples, 638289 words/s, in_qsize 13, out_qsize 2\n",
      "2018-12-05 10:30:35,285 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:35,293 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:35,305 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:35,307 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:35,308 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:35,314 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:35,329 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:35,331 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:35,333 - base_any2vec - INFO - EPOCH - 11 : training on 1570282 raw words (1266305 effective words) took 2.0s, 641712 effective words/s\n",
      "2018-12-05 10:30:36,342 - base_any2vec - INFO - EPOCH 12 - PROGRESS: at 60.08% examples, 719173 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:37,030 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:37,053 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:37,060 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:37,074 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:37,075 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:37,087 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:37,088 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:37,099 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:37,100 - base_any2vec - INFO - EPOCH - 12 : training on 1570282 raw words (1266260 effective words) took 1.8s, 718922 effective words/s\n",
      "2018-12-05 10:30:38,121 - base_any2vec - INFO - EPOCH 13 - PROGRESS: at 52.33% examples, 630814 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:39,139 - base_any2vec - INFO - EPOCH 13 - PROGRESS: at 91.09% examples, 562660 words/s, in_qsize 14, out_qsize 0\n",
      "2018-12-05 10:30:39,250 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:39,275 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:39,323 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:39,331 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:39,351 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:39,379 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:30:39,387 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:39,389 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:39,390 - base_any2vec - INFO - EPOCH - 13 : training on 1570282 raw words (1266099 effective words) took 2.3s, 554054 effective words/s\n",
      "2018-12-05 10:30:40,406 - base_any2vec - INFO - EPOCH 14 - PROGRESS: at 38.37% examples, 486519 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:41,431 - base_any2vec - INFO - EPOCH 14 - PROGRESS: at 84.50% examples, 534279 words/s, in_qsize 13, out_qsize 2\n",
      "2018-12-05 10:30:41,680 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:41,682 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:41,683 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:41,692 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:41,696 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:41,699 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:41,708 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:41,715 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:41,716 - base_any2vec - INFO - EPOCH - 14 : training on 1570282 raw words (1266267 effective words) took 2.3s, 547985 effective words/s\n",
      "2018-12-05 10:30:42,748 - base_any2vec - INFO - EPOCH 15 - PROGRESS: at 39.15% examples, 490600 words/s, in_qsize 16, out_qsize 0\n",
      "2018-12-05 10:30:43,761 - base_any2vec - INFO - EPOCH 15 - PROGRESS: at 91.47% examples, 572089 words/s, in_qsize 12, out_qsize 0\n",
      "2018-12-05 10:30:43,870 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:43,874 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:43,881 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:43,882 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:43,883 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:43,894 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:43,906 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:43,918 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:43,919 - base_any2vec - INFO - EPOCH - 15 : training on 1570282 raw words (1266216 effective words) took 2.2s, 577429 effective words/s\n",
      "2018-12-05 10:30:44,943 - base_any2vec - INFO - EPOCH 16 - PROGRESS: at 55.81% examples, 670423 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:45,875 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:45,877 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:45,895 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:45,904 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:45,907 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:45,912 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:45,920 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:45,927 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:45,928 - base_any2vec - INFO - EPOCH - 16 : training on 1570282 raw words (1266133 effective words) took 2.0s, 631960 effective words/s\n",
      "2018-12-05 10:30:46,937 - base_any2vec - INFO - EPOCH 17 - PROGRESS: at 61.24% examples, 737177 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:47,766 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:47,771 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:47,775 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:47,780 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:47,792 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:47,812 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:47,828 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:47,835 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:47,836 - base_any2vec - INFO - EPOCH - 17 : training on 1570282 raw words (1266283 effective words) took 1.9s, 665550 effective words/s\n",
      "2018-12-05 10:30:48,849 - base_any2vec - INFO - EPOCH 18 - PROGRESS: at 50.00% examples, 623080 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:49,679 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:49,685 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:49,710 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:49,719 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:49,725 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:49,730 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:49,738 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:49,740 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:49,742 - base_any2vec - INFO - EPOCH - 18 : training on 1570282 raw words (1266267 effective words) took 1.9s, 665624 effective words/s\n",
      "2018-12-05 10:30:50,763 - base_any2vec - INFO - EPOCH 19 - PROGRESS: at 43.80% examples, 553192 words/s, in_qsize 15, out_qsize 0\n",
      "2018-12-05 10:30:51,689 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:51,701 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:51,711 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:51,718 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:51,724 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:51,734 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:51,735 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2018-12-05 10:30:51,741 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:51,742 - base_any2vec - INFO - EPOCH - 19 : training on 1570282 raw words (1266222 effective words) took 2.0s, 635420 effective words/s\n",
      "2018-12-05 10:30:52,750 - base_any2vec - INFO - EPOCH 20 - PROGRESS: at 61.63% examples, 748501 words/s, in_qsize 14, out_qsize 1\n",
      "2018-12-05 10:30:53,380 - base_any2vec - INFO - worker thread finished; awaiting finish of 7 more threads\n",
      "2018-12-05 10:30:53,384 - base_any2vec - INFO - worker thread finished; awaiting finish of 6 more threads\n",
      "2018-12-05 10:30:53,387 - base_any2vec - INFO - worker thread finished; awaiting finish of 5 more threads\n",
      "2018-12-05 10:30:53,407 - base_any2vec - INFO - worker thread finished; awaiting finish of 4 more threads\n",
      "2018-12-05 10:30:53,417 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2018-12-05 10:30:53,438 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2018-12-05 10:30:53,452 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:30:53,453 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2018-12-05 10:30:53,454 - base_any2vec - INFO - EPOCH - 20 : training on 1570282 raw words (1266433 effective words) took 1.7s, 742158 effective words/s\n",
      "2018-12-05 10:30:53,455 - base_any2vec - INFO - training on a 31405640 raw words (25324816 effective words) took 40.7s, 621784 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Doc2Vec(vector_size=100, window=10, min_count=1, workers=8, alpha=0.025, min_alpha=0.015, \n",
    "                              epochs=20)\n",
    "\n",
    "#sample=1e-4, negative=5,\n",
    "\n",
    "#shuffling is better (ot needed at each trianing epoch\n",
    "shuffle(tagged_data)\n",
    "#Build vocabulary from a sequence of sentences \n",
    "model.build_vocab(tagged_data)\n",
    "#Update the model’s neural weights from a sequence of sentences\n",
    "model.train(tagged_data, epochs=model.epochs, total_examples=model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"finalmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Baek, Woongki': Doctag(offset=0, word_count=3611, doc_count=1),\n",
       " 'Son, Jae Sung': Doctag(offset=1, word_count=3129, doc_count=1),\n",
       " 'Kim, Jooha': Doctag(offset=2, word_count=933, doc_count=1),\n",
       " 'Kim, Guntae': Doctag(offset=3, word_count=10826, doc_count=1),\n",
       " 'Kim, Chae Un': Doctag(offset=4, word_count=321, doc_count=1),\n",
       " 'Park, Hyesung': Doctag(offset=5, word_count=2059, doc_count=1),\n",
       " 'Chun, Se Young': Doctag(offset=6, word_count=4987, doc_count=1),\n",
       " 'Lim, Sunghoon': Doctag(offset=7, word_count=867, doc_count=1),\n",
       " 'Song, Hyun-Kon': Doctag(offset=8, word_count=10331, doc_count=1),\n",
       " 'Jung, Jee-Hoon': Doctag(offset=9, word_count=7931, doc_count=1),\n",
       " 'Chung, Moses': Doctag(offset=10, word_count=3529, doc_count=1),\n",
       " 'Myung, Kyungjae': Doctag(offset=11, word_count=9148, doc_count=1),\n",
       " 'Lee, Changwook': Doctag(offset=12, word_count=2439, doc_count=1),\n",
       " 'Kim, Jaai': Doctag(offset=13, word_count=6645, doc_count=1),\n",
       " 'Kwon, Daeil': Doctag(offset=14, word_count=4439, doc_count=1),\n",
       " 'Kim, Byungmin': Doctag(offset=15, word_count=1339, doc_count=1),\n",
       " 'Lee, Jae Hwa': Doctag(offset=16, word_count=6095, doc_count=1),\n",
       " 'Kim, Sung-Phil': Doctag(offset=17, word_count=14246, doc_count=1),\n",
       " 'Kwon, Bongsuk': Doctag(offset=18, word_count=1624, doc_count=1),\n",
       " 'Chung, Jin-Ho': Doctag(offset=19, word_count=2671, doc_count=1),\n",
       " 'Oakley, Ian': Doctag(offset=20, word_count=8912, doc_count=1),\n",
       " 'Jo, Wook': Doctag(offset=21, word_count=13482, doc_count=1),\n",
       " 'Cho, Hyungjoon': Doctag(offset=22, word_count=4653, doc_count=1),\n",
       " 'Park, Jung-Hoon': Doctag(offset=23, word_count=1370, doc_count=1),\n",
       " 'Choi, Jin Hyuk': Doctag(offset=24, word_count=223, doc_count=1),\n",
       " 'Choi, Kyudong': Doctag(offset=25, word_count=309, doc_count=1),\n",
       " 'Park, Cheol-Min': Doctag(offset=26, word_count=1848, doc_count=1),\n",
       " 'Park, Young-Woo': Doctag(offset=27, word_count=782, doc_count=1),\n",
       " 'Kang, Joo H.': Doctag(offset=28, word_count=1416, doc_count=1),\n",
       " 'Lee, Ja Yil': Doctag(offset=29, word_count=1610, doc_count=1),\n",
       " 'Park, Lee Soon': Doctag(offset=30, word_count=15659, doc_count=1),\n",
       " 'Moon, Hoi Ri': Doctag(offset=31, word_count=4427, doc_count=1),\n",
       " 'Kyung, Gyouhyung': Doctag(offset=32, word_count=3364, doc_count=1),\n",
       " 'Kim, Hee Reyoung': Doctag(offset=33, word_count=6701, doc_count=1),\n",
       " 'Lee, Sung Kuk': Doctag(offset=34, word_count=4989, doc_count=1),\n",
       " 'Jung, Dooyoung': Doctag(offset=35, word_count=2107, doc_count=1),\n",
       " 'Kim, Chajoong': Doctag(offset=36, word_count=1836, doc_count=1),\n",
       " 'Cho, Jaephil': Doctag(offset=37, word_count=28277, doc_count=1),\n",
       " 'Torbol, Marco': Doctag(offset=38, word_count=3995, doc_count=1),\n",
       " 'Kang, Sarah M.': Doctag(offset=39, word_count=6174, doc_count=1),\n",
       " 'Jung, Chang-Yeol': Doctag(offset=40, word_count=3894, doc_count=1),\n",
       " 'Song, Chang-Keun': Doctag(offset=41, word_count=7635, doc_count=1),\n",
       " 'Lee, Zonghoon': Doctag(offset=42, word_count=12657, doc_count=1),\n",
       " 'Seo, Yongwon': Doctag(offset=43, word_count=7628, doc_count=1),\n",
       " 'Vigneron, Antoine': Doctag(offset=44, word_count=4021, doc_count=1),\n",
       " 'Kim, Jin Young': Doctag(offset=45, word_count=15705, doc_count=1),\n",
       " 'Kim, Kwang S.': Doctag(offset=46, word_count=42203, doc_count=1),\n",
       " 'Jin, Hosub': Doctag(offset=47, word_count=3920, doc_count=1),\n",
       " 'Kim, Hak Sun': Doctag(offset=48, word_count=3417, doc_count=1),\n",
       " 'Sim, Sung-Han': Doctag(offset=49, word_count=9407, doc_count=1),\n",
       " 'Choi, Nam-Soon': Doctag(offset=50, word_count=10025, doc_count=1),\n",
       " 'Hong, Sung You': Doctag(offset=51, word_count=3076, doc_count=1),\n",
       " 'Kim, Kwanpyo': Doctag(offset=52, word_count=4639, doc_count=1),\n",
       " 'Choi, Jaehyouk': Doctag(offset=53, word_count=2467, doc_count=1),\n",
       " 'Kwak, Youngshin': Doctag(offset=54, word_count=5367, doc_count=1),\n",
       " 'Au, Tsz-Chiu': Doctag(offset=55, word_count=1478, doc_count=1),\n",
       " 'Kim, Donghyuk': Doctag(offset=56, word_count=3535, doc_count=1),\n",
       " 'Kim, Hajin': Doctag(offset=57, word_count=6583, doc_count=1),\n",
       " 'Kwak, Ja Hun': Doctag(offset=58, word_count=17591, doc_count=1),\n",
       " 'Kim, Duck Young': Doctag(offset=59, word_count=1118, doc_count=1),\n",
       " 'Kim, KwanMyung': Doctag(offset=60, word_count=1551, doc_count=1),\n",
       " 'Park, Hyung Wook': Doctag(offset=61, word_count=10232, doc_count=1),\n",
       " 'Kim, Yeolib': Doctag(offset=62, word_count=631, doc_count=1),\n",
       " 'Yun, Aaram': Doctag(offset=63, word_count=1173, doc_count=1),\n",
       " 'Min, Seung Kyu': Doctag(offset=64, word_count=3137, doc_count=1),\n",
       " 'Park, Hyeon Keo': Doctag(offset=65, word_count=29013, doc_count=1),\n",
       " 'Joo, Sang Hoon': Doctag(offset=66, word_count=13872, doc_count=1),\n",
       " 'Baek, Joon Sang': Doctag(offset=67, word_count=989, doc_count=1),\n",
       " 'Shin, Myoungsu': Doctag(offset=68, word_count=6280, doc_count=1),\n",
       " 'Self, James': Doctag(offset=69, word_count=1748, doc_count=1),\n",
       " 'Choi, Jaesik': Doctag(offset=70, word_count=4054, doc_count=1),\n",
       " 'Kim, Jingook': Doctag(offset=71, word_count=7595, doc_count=1),\n",
       " 'Lee, Geunsik': Doctag(offset=72, word_count=7836, doc_count=1),\n",
       " 'Kim, Seong-Jin': Doctag(offset=73, word_count=2260, doc_count=1),\n",
       " 'Baek, Jong-Beom': Doctag(offset=74, word_count=20400, doc_count=1),\n",
       " 'Bang, In Cheol': Doctag(offset=75, word_count=17706, doc_count=1),\n",
       " 'Kim, Yunho': Doctag(offset=76, word_count=1120, doc_count=1),\n",
       " 'Baik, Jeong Min': Doctag(offset=77, word_count=8998, doc_count=1),\n",
       " 'Ryu, Dongsu': Doctag(offset=78, word_count=23054, doc_count=1),\n",
       " 'Lee, Kyuho Jason': Doctag(offset=79, word_count=2896, doc_count=1),\n",
       " 'Bae, Joonbum': Doctag(offset=80, word_count=9369, doc_count=1),\n",
       " 'Kim, Sung Youb': Doctag(offset=81, word_count=6873, doc_count=1),\n",
       " 'Sohn, Dong-Seong': Doctag(offset=82, word_count=6954, doc_count=1),\n",
       " 'Oh, Yoon Seok': Doctag(offset=83, word_count=4286, doc_count=1),\n",
       " 'Park, Young S.': Doctag(offset=84, word_count=1225, doc_count=1),\n",
       " 'Tlusty, Tsvi': Doctag(offset=85, word_count=7092, doc_count=1),\n",
       " 'Park, Tae-Eun': Doctag(offset=86, word_count=3112, doc_count=1),\n",
       " 'Lee, Jun Hee': Doctag(offset=87, word_count=3383, doc_count=1),\n",
       " 'Jun, Young Chul': Doctag(offset=88, word_count=3810, doc_count=1),\n",
       " 'Lah, Myoung Soo': Doctag(offset=89, word_count=11260, doc_count=1),\n",
       " 'Lee, Ki-Suk': Doctag(offset=90, word_count=6427, doc_count=1),\n",
       " 'Ki, Hyungson': Doctag(offset=91, word_count=4873, doc_count=1),\n",
       " 'Yang, Seungjoon': Doctag(offset=92, word_count=4214, doc_count=1),\n",
       " 'Park, Tae Joo': Doctag(offset=93, word_count=2068, doc_count=1),\n",
       " 'Lee, Dong Woog': Doctag(offset=94, word_count=3105, doc_count=1),\n",
       " 'Ding, Feng': Doctag(offset=95, word_count=12053, doc_count=1),\n",
       " 'Ahn, Sangjoon': Doctag(offset=96, word_count=915, doc_count=1),\n",
       " 'Cho, Gi-Hyoug': Doctag(offset=97, word_count=2832, doc_count=1),\n",
       " 'Kim, Ju-Young': Doctag(offset=98, word_count=10229, doc_count=1),\n",
       " 'Oh, Jae Eun': Doctag(offset=99, word_count=3742, doc_count=1),\n",
       " 'Lee, Jongwon': Doctag(offset=100, word_count=2141, doc_count=1),\n",
       " 'Kim, Eunhee': Doctag(offset=101, word_count=4840, doc_count=1),\n",
       " 'Son, Hungsun': Doctag(offset=102, word_count=5891, doc_count=1),\n",
       " 'Park, Jaeyeong': Doctag(offset=103, word_count=1012, doc_count=1),\n",
       " 'Lee, Changyong': Doctag(offset=104, word_count=4876, doc_count=1),\n",
       " 'Choi, EunMi': Doctag(offset=105, word_count=5790, doc_count=1),\n",
       " 'Lee, Chang Young': Doctag(offset=106, word_count=8931, doc_count=1),\n",
       " 'Hong, Woonki': Doctag(offset=107, word_count=527, doc_count=1),\n",
       " 'Lee, Jaeseon': Doctag(offset=108, word_count=2909, doc_count=1),\n",
       " 'Ryu, Ja-Hyoung': Doctag(offset=109, word_count=5096, doc_count=1),\n",
       " 'Kwon, Min-Suk': Doctag(offset=110, word_count=3955, doc_count=1),\n",
       " 'Kwon, Oh Hoon': Doctag(offset=111, word_count=6083, doc_count=1),\n",
       " 'Cha, Dong-Hyun': Doctag(offset=112, word_count=5306, doc_count=1),\n",
       " 'Ko, Hyunhyub': Doctag(offset=113, word_count=9710, doc_count=1),\n",
       " 'Kim, Pilwon': Doctag(offset=114, word_count=1631, doc_count=1),\n",
       " 'Kim, Hyoil': Doctag(offset=115, word_count=3324, doc_count=1),\n",
       " 'Kwon, Tae-Hyuk': Doctag(offset=116, word_count=3396, doc_count=1),\n",
       " 'Shin, GwanSeob': Doctag(offset=117, word_count=5389, doc_count=1),\n",
       " 'Kim, Jeong Beom': Doctag(offset=118, word_count=2366, doc_count=1),\n",
       " 'Baig, Chunggi': Doctag(offset=119, word_count=7752, doc_count=1),\n",
       " 'Jeong, Joonwoo': Doctag(offset=120, word_count=1300, doc_count=1),\n",
       " 'Lee, Jacob C.': Doctag(offset=121, word_count=363, doc_count=1),\n",
       " 'Kim, Gun-Ho': Doctag(offset=122, word_count=921, doc_count=1),\n",
       " 'Lee, Chang Hyeong': Doctag(offset=123, word_count=2201, doc_count=1),\n",
       " 'Kim, Ji Hyun': Doctag(offset=124, word_count=11423, doc_count=1),\n",
       " 'Ghim, Cheol-Min': Doctag(offset=125, word_count=2428, doc_count=1),\n",
       " 'Kim, Yong Hwan': Doctag(offset=126, word_count=10287, doc_count=1),\n",
       " 'Moon, Hyungon': Doctag(offset=127, word_count=939, doc_count=1),\n",
       " 'Chung, Dongil': Doctag(offset=128, word_count=1437, doc_count=1),\n",
       " 'Lee, Young-Joo': Doctag(offset=129, word_count=4178, doc_count=1),\n",
       " 'Zhang, Lu': Doctag(offset=130, word_count=1365, doc_count=1),\n",
       " 'Lee, Myunghee': Doctag(offset=131, word_count=1784, doc_count=1),\n",
       " 'Bae, Sung Chul': Doctag(offset=132, word_count=4560, doc_count=1),\n",
       " 'Kim, Je-Hyung': Doctag(offset=133, word_count=2888, doc_count=1),\n",
       " 'Kwak, Sang Kyu': Doctag(offset=134, word_count=13087, doc_count=1),\n",
       " 'Lim, Chiehyeon': Doctag(offset=135, word_count=2230, doc_count=1),\n",
       " 'Park, Jongnam': Doctag(offset=136, word_count=6370, doc_count=1),\n",
       " 'Choi, Young-Ri': Doctag(offset=137, word_count=1745, doc_count=1),\n",
       " 'Jeong, Hoon Eui': Doctag(offset=138, word_count=6646, doc_count=1),\n",
       " 'Park, Jiyoung': Doctag(offset=139, word_count=4211, doc_count=1),\n",
       " 'Lee, Sukbin': Doctag(offset=140, word_count=3370, doc_count=1),\n",
       " 'Kang, Byoung Heon': Doctag(offset=141, word_count=2963, doc_count=1),\n",
       " 'Chung, Il-Sug': Doctag(offset=142, word_count=4633, doc_count=1),\n",
       " 'Kim, Katherine A.': Doctag(offset=143, word_count=3792, doc_count=1),\n",
       " 'Cho, Perter J.': Doctag(offset=144, word_count=824, doc_count=1),\n",
       " 'Bae, Hantaek': Doctag(offset=145, word_count=846, doc_count=1),\n",
       " 'Kim, Young Choon': Doctag(offset=146, word_count=941, doc_count=1),\n",
       " 'Kim, So Youn': Doctag(offset=147, word_count=2526, doc_count=1),\n",
       " 'Hur, Min Sup': Doctag(offset=148, word_count=7223, doc_count=1),\n",
       " 'Min, Kyung-Tai': Doctag(offset=149, word_count=3113, doc_count=1),\n",
       " 'Choi, Kyoung Jin': Doctag(offset=150, word_count=8767, doc_count=1),\n",
       " 'Cho, Jaeweon': Doctag(offset=151, word_count=22403, doc_count=1),\n",
       " 'Kim, Jae-Ick': Doctag(offset=152, word_count=1988, doc_count=1),\n",
       " 'Lim, Hankwon': Doctag(offset=153, word_count=5366, doc_count=1),\n",
       " 'Kwon, Min Sang': Doctag(offset=154, word_count=1976, doc_count=1),\n",
       " 'Jun, Yubin': Doctag(offset=155, word_count=1968, doc_count=1),\n",
       " 'Sim, Jae-Young': Doctag(offset=156, word_count=6144, doc_count=1),\n",
       " 'Im, Jungho': Doctag(offset=157, word_count=13833, doc_count=1),\n",
       " 'Jang, Ji-Wook': Doctag(offset=158, word_count=5175, doc_count=1),\n",
       " 'Byon, Chan': Doctag(offset=159, word_count=11375, doc_count=1),\n",
       " 'Park, Kibog': Doctag(offset=160, word_count=4216, doc_count=1),\n",
       " 'An, Kwangjin': Doctag(offset=161, word_count=5502, doc_count=1),\n",
       " 'Kwon, Oh-Sang': Doctag(offset=162, word_count=1075, doc_count=1),\n",
       " 'Jeong, Won-Ki': Doctag(offset=163, word_count=5462, doc_count=1),\n",
       " 'Jeong, Kyeong-Min': Doctag(offset=164, word_count=563, doc_count=1),\n",
       " 'Chang, Jiwon': Doctag(offset=165, word_count=1880, doc_count=1),\n",
       " 'Comuzzi, Marco': Doctag(offset=166, word_count=6493, doc_count=1),\n",
       " 'Park, Noejung': Doctag(offset=167, word_count=12215, doc_count=1),\n",
       " 'Lee, Jongeun': Doctag(offset=168, word_count=7311, doc_count=1),\n",
       " 'Lee, Junghye': Doctag(offset=169, word_count=1054, doc_count=1),\n",
       " 'Amblard, Francois': Doctag(offset=170, word_count=4572, doc_count=1),\n",
       " 'Mitchell, Robert J.': Doctag(offset=171, word_count=7488, doc_count=1),\n",
       " 'Choi, Sung-Deuk': Doctag(offset=172, word_count=10842, doc_count=1),\n",
       " 'Kim, Jiyun': Doctag(offset=173, word_count=1260, doc_count=1),\n",
       " 'Ruoff, Rodney S.': Doctag(offset=174, word_count=45307, doc_count=1),\n",
       " 'Schultz, Thomas': Doctag(offset=175, word_count=9696, doc_count=1),\n",
       " 'Kim, Sungil': Doctag(offset=176, word_count=1065, doc_count=1),\n",
       " 'Lee, Deokjung': Doctag(offset=177, word_count=7744, doc_count=1),\n",
       " 'Lee, Kyunghan': Doctag(offset=178, word_count=5395, doc_count=1),\n",
       " 'Cho, Yoon-Kyoung': Doctag(offset=179, word_count=11510, doc_count=1),\n",
       " 'Granick, Steve': Doctag(offset=180, word_count=25393, doc_count=1),\n",
       " 'Jang, Jaesung': Doctag(offset=181, word_count=3091, doc_count=1),\n",
       " 'Park, Sung Soo': Doctag(offset=182, word_count=4887, doc_count=1),\n",
       " 'Kim, Byeong-Su': Doctag(offset=183, word_count=13079, doc_count=1),\n",
       " 'Bielawski, Christopher W.': Doctag(offset=184, word_count=25573, doc_count=1),\n",
       " 'Kim, Namhun': Doctag(offset=185, word_count=5389, doc_count=1),\n",
       " 'Kim, Taesung': Doctag(offset=186, word_count=9144, doc_count=1),\n",
       " 'Seok, Sang Il': Doctag(offset=187, word_count=12945, doc_count=1),\n",
       " 'Lee, Changsoo': Doctag(offset=188, word_count=8883, doc_count=1),\n",
       " 'Scharer, Orlando D.': Doctag(offset=189, word_count=8395, doc_count=1),\n",
       " 'Yang, Changduk': Doctag(offset=190, word_count=13973, doc_count=1),\n",
       " 'Lee, Myong-In': Doctag(offset=191, word_count=9663, doc_count=1),\n",
       " 'Shin, Hyeon Suk': Doctag(offset=192, word_count=7088, doc_count=1),\n",
       " 'Kim, Yung Sam': Doctag(offset=193, word_count=2646, doc_count=1),\n",
       " 'Lee, Yongjae': Doctag(offset=194, word_count=434, doc_count=1),\n",
       " 'Chae, Han Gi': Doctag(offset=195, word_count=5343, doc_count=1),\n",
       " 'Kim, MinChung': Doctag(offset=196, word_count=1010, doc_count=1),\n",
       " 'Kwon, H. Moo': Doctag(offset=197, word_count=13533, doc_count=1),\n",
       " 'Ko, Myunggon': Doctag(offset=198, word_count=3128, doc_count=1),\n",
       " 'Kwon, Young-Nam': Doctag(offset=199, word_count=6745, doc_count=1),\n",
       " 'Song, Myoung Hoon': Doctag(offset=200, word_count=5496, doc_count=1),\n",
       " 'Yoon, Eisung': Doctag(offset=201, word_count=1182, doc_count=1),\n",
       " 'Park, Young-Bin': Doctag(offset=202, word_count=13124, doc_count=1),\n",
       " 'Byun, Gangil': Doctag(offset=203, word_count=3643, doc_count=1),\n",
       " 'Jang, Bongsoo': Doctag(offset=204, word_count=2206, doc_count=1),\n",
       " 'Jung, Kooyul': Doctag(offset=205, word_count=1540, doc_count=1),\n",
       " 'Lee, Jae Sung': Doctag(offset=206, word_count=34083, doc_count=1),\n",
       " 'Kang, Sang Hoon': Doctag(offset=207, word_count=2664, doc_count=1),\n",
       " 'Hwang, Sung Ju': Doctag(offset=208, word_count=1953, doc_count=1),\n",
       " 'Kwak, Kyujin': Doctag(offset=209, word_count=2963, doc_count=1),\n",
       " 'Shin, Hyung-Joon': Doctag(offset=210, word_count=3811, doc_count=1),\n",
       " 'Ji, Wooseok': Doctag(offset=211, word_count=2653, doc_count=1),\n",
       " 'Jang, Ji-Hyun': Doctag(offset=212, word_count=6951, doc_count=1),\n",
       " 'Noh, Sam H.': Doctag(offset=213, word_count=9904, doc_count=1),\n",
       " 'Park, Chan Young': Doctag(offset=214, word_count=2862, doc_count=1),\n",
       " 'Bien, Franklin': Doctag(offset=215, word_count=8236, doc_count=1),\n",
       " 'Choi, Jang Hyun': Doctag(offset=216, word_count=5544, doc_count=1),\n",
       " 'Cha, Chaenyung': Doctag(offset=217, word_count=4971, doc_count=1),\n",
       " 'In, Yongkyoon': Doctag(offset=218, word_count=7674, doc_count=1),\n",
       " 'Kim, Kyung Rok': Doctag(offset=219, word_count=4853, doc_count=1),\n",
       " 'Jang, Hyunjin': Doctag(offset=220, word_count=597, doc_count=1),\n",
       " 'Fynan, Douglas A.': Doctag(offset=221, word_count=1093, doc_count=1),\n",
       " 'Kim, Jae Joon': Doctag(offset=222, word_count=2801, doc_count=1),\n",
       " 'Lee, Sang-Young': Doctag(offset=223, word_count=13219, doc_count=1),\n",
       " 'Choe, Wonyoung': Doctag(offset=224, word_count=3604, doc_count=1),\n",
       " 'Rohde, Jan-Uwe': Doctag(offset=225, word_count=5021, doc_count=1),\n",
       " 'Shin, Heungjoo': Doctag(offset=226, word_count=6038, doc_count=1),\n",
       " 'Yoo, Jung-Woo': Doctag(offset=227, word_count=4585, doc_count=1),\n",
       " 'Yang, Hyun Jong': Doctag(offset=228, word_count=4519, doc_count=1),\n",
       " 'Lee, Jiseok': Doctag(offset=229, word_count=1867, doc_count=1),\n",
       " 'Bhak, Jong': Doctag(offset=230, word_count=13879, doc_count=1),\n",
       " 'Sun, Hae-sang': Doctag(offset=231, word_count=169, doc_count=1),\n",
       " 'Kang, Seokhyeong': Doctag(offset=232, word_count=4178, doc_count=1),\n",
       " 'Lim, Chunghun': Doctag(offset=233, word_count=3570, doc_count=1),\n",
       " 'Chung, Jibum': Doctag(offset=234, word_count=893, doc_count=1),\n",
       " 'Kwon, Taejoon': Doctag(offset=235, word_count=2900, doc_count=1),\n",
       " 'Choi, Young Rok': Doctag(offset=236, word_count=1037, doc_count=1),\n",
       " 'Jung, Woonggyu': Doctag(offset=237, word_count=11145, doc_count=1),\n",
       " 'Grzybowski, Bartosz A.': Doctag(offset=238, word_count=20328, doc_count=1),\n",
       " 'Kang, Hyun-Wook': Doctag(offset=239, word_count=4725, doc_count=1),\n",
       " 'Moon, Jun': Doctag(offset=240, word_count=2507, doc_count=1),\n",
       " 'Ko, Sungahn': Doctag(offset=241, word_count=1368, doc_count=1),\n",
       " 'Park, Jang-Ung': Doctag(offset=242, word_count=6181, doc_count=1),\n",
       " 'Kim, Molan': Doctag(offset=243, word_count=456, doc_count=1),\n",
       " 'Oh, Joo Hwan': Doctag(offset=244, word_count=1924, doc_count=1),\n",
       " 'Cho, Kyung Hwa': Doctag(offset=245, word_count=10383, doc_count=1),\n",
       " 'Kim, Jeongseob': Doctag(offset=246, word_count=1222, doc_count=1),\n",
       " 'Kwon, Soon-Yong': Doctag(offset=247, word_count=7628, doc_count=1),\n",
       " 'Hong, Hwajung': Doctag(offset=248, word_count=1780, doc_count=1),\n",
       " 'Seo, Kwanyong': Doctag(offset=249, word_count=6624, doc_count=1),\n",
       " 'Woo, Hangyun': Doctag(offset=250, word_count=726, doc_count=1),\n",
       " 'Kang, Seok Ju': Doctag(offset=251, word_count=6318, doc_count=1),\n",
       " 'Joo, Changhee': Doctag(offset=252, word_count=8545, doc_count=1),\n",
       " 'Lee, Semin': Doctag(offset=253, word_count=4732, doc_count=1),\n",
       " 'Oh, Hyondong': Doctag(offset=254, word_count=4797, doc_count=1),\n",
       " 'Nam, Beomseok': Doctag(offset=255, word_count=4232, doc_count=1),\n",
       " 'Kang, Sebyung': Doctag(offset=256, word_count=6163, doc_count=1),\n",
       " 'Nam, Dougu': Doctag(offset=257, word_count=2672, doc_count=1)}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.doctags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:34:14,510 - keyedvectors - INFO - precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Self, James', 0.7371705770492554),\n",
       " ('Baek, Joon Sang', 0.7270042300224304),\n",
       " ('Kim, KwanMyung', 0.6994534730911255),\n",
       " ('Lee, Jacob C.', 0.6763160228729248),\n",
       " ('Lim, Sunghoon', 0.6498798131942749),\n",
       " ('Kyung, Gyouhyung', 0.648121178150177),\n",
       " ('Zhang, Lu', 0.6405069231987),\n",
       " ('Kim, Yeolib', 0.6356873512268066),\n",
       " ('Lim, Chiehyeon', 0.6298955678939819),\n",
       " ('Kim, Molan', 0.6261118054389954)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar('Kim, Chajoong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 10:34:24,572 - keyedvectors - INFO - storing 258x100 projection weights into doc_tensor.w2v\n"
     ]
    }
   ],
   "source": [
    "model.save_word2vec_format('doc_tensor.w2v', doctag_vec=True, word_vec=False, binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "profname = list(model.docvecs.doctags.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outfiletsv.tsv', 'w') as file_vector:\n",
    "    with open('outfiletsvmeta.tsv', 'w') as file_metadata:\n",
    "        for word in model.docvecs.index2entity:\n",
    "            file_metadata.write(word + '\\n')\n",
    "            vector_row = '\\t'.join(str(x) for x in model[word])\n",
    "            file_vector.write(vector_row + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prof2vec.tsv','w') as file_vector:\n",
    "    for word in model.docvecs.index2entity:\n",
    "        vector_row = '\\t'.join(str(x) for x in model[word])\n",
    "        file_vector.write(vector_row + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-05 09:45:52,754 - word2vec2tensor - INFO - running /Users/seungheondoh/anaconda3/envs/python3.6/lib/python3.6/site-packages/gensim/scripts/word2vec2tensor.py -i doc_tensor.w2v -o prof2vec\n",
      "2018-12-05 09:45:52,756 - utils_any2vec - INFO - loading projection weights from doc_tensor.w2v\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid vector on line 0 (is this really the text format?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/gensim/scripts/word2vec2tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"running %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mword2vec2tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished running %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/gensim/scripts/word2vec2tensor.py\u001b[0m in \u001b[0;36mword2vec2tensor\u001b[0;34m(word2vec_model_path, tensor_filename, binary)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0moutfiletsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_filename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_tensor.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0moutfiletsvmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_filename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_metadata.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid vector on line 0 (is this really the text format?)"
     ]
    }
   ],
   "source": [
    "%run /Users/seungheondoh/anaconda3/envs/python3.6/lib/python3.6/site-packages/gensim/scripts/word2vec2tensor.py -i doc_tensor.w2v -o prof2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prof2vec_metadata.tsv','w') as w:\n",
    "    w.write('Titles\\tdepartment\\n')\n",
    "    for i,j in zip(testdf.Name, testdf.department):\n",
    "        w.write(\"%s\\t%s\\n\" % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungheondoh/anaconda3/envs/python3.6/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "final = []\n",
    "for i in profname:\n",
    "    count = 0\n",
    "    mostsimilar = model.docvecs.most_similar(i)\n",
    "    for a,b in mostsimilar:\n",
    "        if a.split('/')[1] == i.split('/')[1]:\n",
    "            count += 1\n",
    "        else:\n",
    "            pass\n",
    "    final.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametertunning(v):\n",
    "    model = gensim.models.Doc2Vec(vector_size=v, window=10, min_count=1, workers=8, alpha=0.025, min_alpha=0.015, \n",
    "                              epochs=20)\n",
    "    profname = list(model.docvecs.doctags.keys())\n",
    "    final = []\n",
    "    for j in profname:\n",
    "        count = 0\n",
    "        mostsimilar = model.docvecs.most_similar(j)\n",
    "        for a,b in mostsimilar:\n",
    "            if a.split('/')[1] == i.split('/')[1]:\n",
    "                count += 1\n",
    "            else:\n",
    "                pass\n",
    "        final.append(count)\n",
    "        average = sum(final)/len(final)\n",
    "        return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
