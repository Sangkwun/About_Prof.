{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram word2vec\n",
    "\n",
    "이 노트북은, TensorFlow 의 word2vec algorithm중 skip-gram의 아키텍처를 설명하기 위함입니다. 당신은 자연어처리의 단어의 embedding 개념을 알게 될 겁니다.\n",
    "\n",
    "\n",
    "## Readings\n",
    "\n",
    "다음과 같은 참고자료가 있습니다. \n",
    "* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of word2vec from Chris McCormick \n",
    "* [First word2vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\n",
    "* [NIPS paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for word2vec also from Mikolov et al.\n",
    "* An [implementation of word2vec](http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/) from Thushan Ganegedara\n",
    "* TensorFlow [word2vec tutorial](https://www.tensorflow.org/tutorials/word2vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "#### Sparse Matrix의 문제점\n",
    "\n",
    "텍스트의 단어를 다룰 때, 각 단어에 대해 하나씩, 수만 개의 클래스를 예측하게됩니다. 이 단어들을 한 번 핫 인코딩하는 것은 대단히 비효율적입니다. 하나의 요소를 1로 설정하고 나머지 50,000을 0으로 설정합니다. 첫 번째 숨겨진 레이어로가는 행렬 곱셈은 거의 모든 결과 값이 0이됩니다. 이것은 엄청난 계산 낭비입니다.\n",
    "\n",
    "![one-hot encodings](assets/one_hot_encoding.png)\n",
    "\n",
    "이 문제를 해결하고 네트워크의 효율성을 크게 높이기 위해 임베딩이라고하는 것을 사용합니다. Embedding은 fully connected layer입니다. 이 레이어를 this layer the embedding layer 가중치는 the weights are embedding weights.라고 합니다. 대신에 우리는 임베딩 레이어와 히든 레이어와의 곱을 건너뜁니다. 가중치 메트릭스를 통해서이죠. 원 핫 코드 벡터를 행렬에 곱하면 \"on\"입력 단위의 인덱스에 해당하는 행이 반환되기 때문에 이러한 연산이 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup table\n",
    "\n",
    "![lookup](assets/lookup_matrix.png)\n",
    "\n",
    "우리는 행렬곱셈 대신에, 가중치 매트릭스를 lookup table을 사용합니다. 우리는 id 값에 단어를 인코딩합니다. 예를 들어 \"heart\"는 958로, \"mind\"는 18094로 인코딩됩니다. 그런 다음 \"heart\"에 대한 숨겨진 레이어 값을 얻으려면 삽입 행렬의 958 번째 행을 가져옵니다. 이 과정을 **embedding lookup** 이라고 하며, 숨겨진 유닛을 **embedding dimension**라고 합니다.\n",
    "\n",
    "<img src='assets/tokenize_lookup.png' width=500>\n",
    " \n",
    "The embedding lookup table 은 그냥 가중치 매트릭스라고 생각하면 됩니다. 임베딩 레이어는 hidden layer입니다. lookup 은 행렬곱의 shortcut이라고 생각하면 됩니다. lookup table 의 학습은 가중치 행렬과 마찬가지로 진행됩니다.\n",
    "<br>\n",
    "\n",
    "Embedding은 물론 단어에 사용되는 것은 아닙니다. 방대한 수의 클래스가있는 모델에는이 모델을 사용할 수 있습니다. **Word2Vec** 라는 특정 유형의 모델은 의미 적 의미를 포함하는 단어의 벡터 표현을 찾기 위해 임베디드 레이어를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "word2vec 알고리즘은 단어를 나타내는 벡터를 찾아 훨씬 더 효율적인 표현을 찾습니다. 이 벡터는 또한 단어에 대한 의미론적 정보를 포함합니다. \"검은 색\", \"흰색\"및 \"빨간색\"과 같은 유사한 문맥에 나타나는 단어는 서로 가깝게 벡터를 갖습니다. CBOW (Continuous Bag-Of-Words) 및 Skip-gram을 구현하는 데는 두 가지 아키텍처가 있습니다.\n",
    "\n",
    "<img src=\"assets/word2vec_architectures.png\" width=\"500\">\n",
    "\n",
    "Skip-gram 아키텍처의 경우, 단어를 둘러싼 단어를 예측합니다. 비슷한 맥락에서 나타나는 단어 표현을 배우기 위해 네트워크를 훈련시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [text8 dataset](http://mattmahoney.net/dc/textdata.html), a file of cleaned up Wikipedia articles from Matt Mahoney."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "\n",
    "dataset_folder_path = 'data'\n",
    "dataset_filename = 'text8.zip'\n",
    "dataset_name = 'Text8 Dataset'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(dataset_filename):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset_name) as pbar:\n",
    "        urlretrieve(\n",
    "            'http://mattmahoney.net/dc/text8.zip',\n",
    "            dataset_filename,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with zipfile.ZipFile(dataset_filename) as zip_ref:\n",
    "        zip_ref.extractall(dataset_folder_path)\n",
    "        \n",
    "with open('data/text8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "전처리과정은 모형의 학습을 돕기위해 진행됩니다 `preprocess` 함수는 임의의 구두점을 토큰으로 덮어주기 때문에 마침표는 `<PERIOD>`로 바뀝니다. 이 데이터 세트에는 마침표가 없지만 다른 NLP 문제에 도움이됩니다. 또한 데이터 집합에서 5 회 이하로 나타나는 모든 단어를 제거합니다. 이렇게하면 데이터의 노이즈로 인한 문제를 크게 줄이고 벡터 표현의 품질을 향상시킬 수 있습니다. 프로젝트 성격에 따라 적당히 수정하시길!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']\n"
     ]
    }
   ],
   "source": [
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 16680599\n",
      "Unique words: 63641\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어를 id로, 그리고 정수를 단어로 mapping 시켜 dictionary를 제작해야합니다. id 값은 정수로 내림차순으로 할당되므로 빈번한 단어는 0번째에 해당되며, int_words에 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look-up table 구축, tokenize 된 단어를 id 에 맵핑한다 생각해라\n",
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "단어들중 빈출단어와, 문장 내 의미를 형성하지 않는 단어를 제거하는 것이 필요합니다. 일부 소음을 제거하고 더 정확한 표현을 할 수 있습니다.\n",
    "\n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "여기서 $t$ 는 threshold를 잡아주는 변수입니다. $f(w_i)$ 는 단어 $w_i$가 전체 데이터셋에서 가지는 빈도를 당담합니다.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "threshold = 1e-5\n",
    "# 단어의 갯수를 샌다.\n",
    "word_counts = Counter(int_words)\n",
    "# 총 단어\n",
    "total_count = len(int_words)\n",
    "# 단어 빈도를 hash 형태로\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "# 확률 표현을 통해 threshold 계산.\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어데이터들을 network에 넣어줘야한다. 우리는 모든 단어를 $C$ 크기의 윈도우에 표현하고 싶습니다.\n",
    "\n",
    "From [Mikolov et al.](https://arxiv.org/pdf/1301.3781.pdf): \n",
    "\n",
    "더 먼 단어는 일반적으로 현재 단어와 관련이 거의 없으므로, 우리는 더 적은 더 적은 weight를 주어야합니다. 만약 우리가 윈도우 사이즈로 5로 담는다면 $C = 5$, 각 학습되는 단어들은 랜덤한 단어를 배정받는다. $R$ in range $< 1; C >$, 그리고 $R$ words은 이전 데이터로부터 학습이되며 그리고 미래로부터 학습되어 현재의 레이블을 결정합니다.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    # 시작 node + 다음 node\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 네트워크에 대한 배치를 반환하는 함수입니다. 아이디어는`batch_size` 단어를 단어 목록에서 가져 오는 것입니다. 그런 다음 각각의 단어에 대해 window에서 대상 단어를 가져옵니다. 따라서 input-target pair당 하나의 행을 만듭니다. 이러한 generate function은 메모리를 절약해 줍니다.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph\n",
    "\n",
    "From [Chris McCormick's blog](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/), we can see the general structure of our network.\n",
    "![embedding_network](./assets/skip_gram_net_arch.png)\n",
    "\n",
    "input 단어는 one-hot encoding 되어 백터형태로 전달됩니다. 그렇다면 선형단위의 숨겨진 레이어로 이동한 다음 softmax 레이어로 이동합니다. softmax 레이어를 사용하여 정규화된 것 처럼 사용이 됩니다.\n",
    "<br>\n",
    "\n",
    "이 아디이는 hidden layer weight matrix를 학습하여 단어를 효율적으로 표현하기 위함입니다. 또한 우리는 softmax layer를 무시할 수 있습니다. 왜냐하면 이 네트워크를 예측하는게 주 목적이 아니기 떄문입니다. 우리의 목표는 embedding martix입니다. 그래서 우리는 데이터셋에서 구축한다른 network를 사용할 수 있습니다.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The embedding matrix 는 단어의 개수와 숨겨진 레이어에 의해서 결정된다. 그래서 만약 10000개 단어와 300개 hidden units이 있다면 matrix 사이즈는 $10,000 \\times 300$ 가 된다. 만약 우리가 토큰화된 단어데이터를 input으로 사용한다면, id값으로 input으로 된다.\n",
    "\n",
    "> [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) that does this lookup for us. You pass in the embedding matrix and a tensor of integers, then it returns rows in the matrix corresponding to those integers. \n",
    "\n",
    "\n",
    "> Below, set the number of embedding features you'll use (200 is a good start), create the embedding matrix variable, and use `tf.nn.embedding_lookup` to get the embedding tensors. For the embedding matrix, I suggest you initialize it with a uniform random numbers between -1 and 1 using [tf.random_uniform](https://www.tensorflow.org/api_docs/python/tf/random_uniform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding = 200 # Number of embedding features \n",
    "with train_graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 네트워크들은 softmax layer를 통해서 학습합니다. 이는 각 입력에 대해 작은 변화에도 각 파라미터들이 학습함을 의미합니다.이것은 매우 네트워크를 비효율적으로 만들게 됩니다. 우리는 작은 부분집합만을 학습시키면서, softmax layer에서의 손실로만 근사시킬 수 있습니다. 우리는 올바른 레이어에 대해서만 업데이터를 시킬 뿐만 아니라, 비 정확한 레이블에 대해서는 학습을 줄여나갈 것입니다. 이것이 네거티브 샘플링입니다. [\"negative sampling\"](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). 텐서플로우는 다음과 같이 구현되어 있습니다., [`tf.nn.sampled_softmax_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of negative labels to sample\n",
    "n_sampled = 100\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_vocab)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "여기서 우리는 몇 가지 일반적인 단어와 드문 단어를 선택하려고합니다. 그런 다음 가장 가까운 단어를 인쇄합니다. 우리의 임베딩 테이블이 의미론적 의미가 비슷한 단어를 그룹화하고 있는지 확인하는 좋은 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: checkpoints: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 500\n",
    "window_size = 10\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 500 == 0:\n",
    "                # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = int_to_vocab[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = int_to_vocab[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습된 네트워크를 저장해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the word vectors\n",
    "\n",
    "아래에서는 T-SNE을 사용하여 우리의 고차원 단어 벡터가 어떻게 집합되는지 시각화합니다. T-SNE는이 벡터를 2 차원으로 투영하는 데 사용되며 로컬 구조를 보존합니다. T-SNE 및 고차원 데이터를 시각화하는 다른 방법에 대해 자세히 알아 보려면 [Christopher Olah] (http://colah.github.io/posts/2014-10-Visualizing-MNIST/)을 확인하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 500\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
